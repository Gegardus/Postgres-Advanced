                                                                PostgreSQL: Advanced Queries

https://medium.com/swlh/learn-sql-joins-once-and-for-all-d5d9078eee7c

INTRO

Using the exercise files

- If you'd like to follow along with me throughout this course, then you can download the same exercise files that I'll be using from the course overview page. I have placed a copy of them here on my desktop, but you can save them wherever is most convenient for you. Now inside of the main folder is a folder for each chapter, and inside of here are the individual script files that show the completed SQL code from the end of each movie. In the chapter zero folder, there's also a script that'll set up the sample database that I'll be using for demonstrations. To set that up, just open up that file and copy the entire contents to your clipboard. Then we can jump into the Postgres SQL query editor. I'll be using PG admin throughout the course, but you can use whatever management interface you prefer. First, we need to create the database. To do that, I'll send the command create database two underscore trees. I'll execute the query, and that'll create the database on my server. Then I can close this query window, and not save any changes to the script. I'll right click on the server and choose refresh, and that should show me the new two trees database. I'll then click on it to connect. Next, we'll open up a new query editor, attached to the two trees database, and I'll paste in the contents of the script from the exercise files. Then, we'll execute the entire script. That'll go through and set up all of the tables and fill in a bunch of data rows. We can verify that everything worked by right clicking on the two trees database and choosing refresh. Then you should see a couple of new schemas. We have the inventory schema with the categories and products table, as well as the sales schema, that includes customers, order lines and orders. So these are the data tables that we'll be working with throughout the course. Now, the only other change that I've made to my system is to streamline what's shown on the screen in order to remove a number of items that we won't be working with. This step is optional, but to get the exact same configuration that I'm using, here inside of PG admin, go up to the file menu, choose preferences. Then in the browser group, click on nodes. I've gone through all of these nodes and turn everything off except for columns, constraints, databases. And I'll scroll through here to find indexes, as well as schemas and tables. So go ahead and toggle all those off except for those few items, and you'll have the same streamlined interface that we're seeing here on my screen. So now you're all set up and ready to explore PostgreSQL's queries and functions.

OBTAIN SUMMARY STASTICS

Using GROUP BY to aggregate data rows

- A SQL select statement is broken down into various clauses and functions that determine the output results. The most basic database query uses the select clause to identify the columns that you want returned from a table, and the from clause to specify the actual table itself. For instance, I'm going to select the two trees database in my PostgreSQL server, and then click on this button here in PG admin to open up a new query tool. Now, if I wanted to see a couple of columns from the products table, I could write out the query like this. We'll select the product name, category ID, size, and price columns from the inventory.products table. Then on the toolbar, I can come up and press this play button, or press the F five shortcut key to execute the query. This returns to the four columns that I asked for and all of the data for every product in the table. In this table we have information on 114 different products, so the query results show 114 rows. You can scroll through the results to see all of them, and we can see that they come all the way down to 114. Now, in order to filter this results set down to only show specific rows, we can add in a where clause with a criteria. All data rows that meet the criteria will be returned and all rows that don't meet the criteria are excluded. For instance, if I only wanted to see the products that had a price over $20, I can come here on to line number three and add in the where clause, and say where price is greater than 20. This time when I press the F five key to execute the query, we'll see these results and I have a total of 37 rows now. These are the 37 products with a price over $20. One row is displayed for every product that meets those criteria. Filtering your data with a where clause could help you explore the information that is stored in the database. Another way that we can get to a better understanding of our data is to start grouping rows together based off of common attributes. For instance, let's analyze our product information based off of the size of each product. I'll start another query here on line number five. I'll start by writing out a select statement that pulls only that column from the database. Now, in order to execute only the second query, I'll make sure that I highlight it first and then press the play button or press the F five shortcut key. That returns one row for every product in the table again, so 114 rows are displayed, but it's only showing me these size information. You can see that there's lots of products that have the same size. We have eight ounce products, 32 ounce products, 64 ounce products and so on. You might like to know how many different size classifications there are in the product data. We can do that by adding in a group by clause to the query, which will take all 114 rows from this result set and combine them into a single row for each size. To do that, I'll add in line here on to line number seven and we'll type in group by size. Now, when I execute the query, we'll see that result. And I can see that I have a total of eight rows here with the different size products. Now it might be nice to see this information in numerical order, so I'll also add in a sort to the result. On line number eight, I'll add in order by size DESC four descending. Once again, I'll highlight lines five down through eight, execute the query and I can see them in a better order. So now the largest size appears at the top and the smallest size appears at the bottom. In a group by query, each of these rows acts as a container for all of our original data. So even though we're only seeing eight rows right now, information on all 114 products is still available. You can think of the group by query, like adding in rows of data to different buckets. All of the eight ounce products go into that bucket. And they're separated from all of these 16 ounce products. Now that we have these buckets, we can look inside each one and do things with all of the rows of data that each bucket contains. The most common thing that you might want to know is how many products are in each size category. We can find that information by adding in another column to the results. Remember that columns are defined in the select clause. So we can add in a counting function up here. On my number five, after the select size, I'll type in a comma and I'll type in a count function. The accounting function needs to know what to count. And in this case, we just want to count all of the records that are in each bucket. So inside a parentheses I'll type in an asterisk. Now when I run the query by highlighting lines five through eight, we'll see we have a new column here that has the count of the number of products inside of each size category. There are a total of 18 products in our 128 ounce size category, and only two products in our four ounce product size. To clean up the display and make these column headers a little bit more descriptive, we can add in some aliases. We'll do that with the as keyword. So after the word size, I'll type in the as keyword and in double quotation marks, we'll type in a new alias product size. We'll do the same thing to our accounting function, I'll type in the as keyword after that, and in double quotation marks number of products. Now when I execute this query, we'll see that we have better names here at the top. This clarifies what each column is displaying. So now we have the number of products in each of our product size categories. Finally, we can limit the rows that are returned in a group by query, just like we filter out rows from a standard select query. When filtering groups, you'll use the having clause. For instance, if you only wanted to see the groups that have over 10 products, you can come up here into the query and I'll make a new line after the group by clause, and I'll type having a count of star greater than 10. Now, when I execute this query, any rows or any groups that have less than 10 products versus this one here, the 12 ounce size, and these two, the six ounce and four ounce products will be removed. Let me execute the query, and we'll see that's exactly the result that we get. Now we only see a total of five different size categories that all have over 10 products in them. Now it's important to note here that when the query is processed, the alias names are defined at the very last step, even though they're written out in our query at the very beginning. This means that you'll need to reference the original column name before aliasing in the having clause. That's why we're using having counter star here on line number eight, instead of having number of products. If you try and write it out as having number of products greater than 10, the query will return an error. So with these group by clauses, you can start to combine your data together based off of common attributes, giving you a different dimension for your analysis work, and it can allow you to better understand the scope of your data.

Obtain general-purpose aggregate statistics

- Once we have individual data rows collected into buckets with the group by clause, we can start to perform some statistical analyses within those groups. We've already seen how to count up the number of rows within a group with the count function. There are a number of additional functions that operate in the same way. Let's start by taking a look at our products and their prices. We'll execute a query that selects the sku, product name, size, and price columns from inventory.products. This returns a row for all 114 products in inventory. Each of the different olive oil products are available in multiple sizes and at different prices. We can use a group by query to combine all of this data based off of the type of oil, and then perform some aggregate calculations to get a better feel for the range of prices for each oil type. We'll start by writing out the group by query with the counting function that we saw previously. This query will retrieve the product name and a count of all the rows within the products table and we're grouping by the product name. Let me highlight lines four through six and it'll execute the query, and we'll see the following results. This groups all of the products by their oil type and displays how many products are in each category. For instance, the virgin olive oil product has five products within that category and the mandarin-infused extra virgin oil only has three products. Like the counting function, PostgreSQL supports other aggregate calculations to find the minimum, maximum, and average values within each of these groups. To find the highest priced product within each oil type, we can add in an additional column up in the select clause. I'll come up here after the counting function, I'll type in a comma to add in a third column, and I'll call this max of price. Unlike the counting function, these additional aggregate functions need to know exactly what column from the original table we want to evaluate. In this case, we're interested in finding the highest price. That's why we need to include the name price inside of the max function. When I highlight these three lines and execute the query, we'll get that additional column in the results where I can see the maximum price within each product name category. The highest price product in the virgin oil is 24.99 and the highest priced mandarin-infused oil is 16.99. You can use the max function to just as easily find the highest size within each oil category. I'll come up here to the top. I'll type in a comma after max price, and this time, I'll type in max size. Once again, I'll highlight these three lines and execute the query. We get an additional column in the results and I can see the highest size in each of the oil categories. Now, at this point, we have two columns with the name max and this can get pretty confusing. So, it's best to clarify the results of our calculations with some custom aliases. I'm going to start to break these out onto their own line, just to make things easier to read. So I'll say, select product name, comma, and then on the next line, count of star, comma, and I'll give this an alias of as number of products. Then, I'll move my max price calculation down to the next line and I'll give it an alias of highest price. And finally, I'll move the max size calculation down and give it an alias, largest size. Now and I run this query here, we'll see the results with the better names at the top. Now since we have a max function, you might assume that there's also a min function to find the smallest value within a group, and you'd be absolutely correct in that assumption. We can retrieve the lowest price for each group with a min function. I'll add a new line number eight, here, and I'll type in min of price, and I'll just give it an alias as lowest price. There's also an average function that we could use called AVG. We'll apply that to our price data as well. This will return the average price from all of the products within each group. Now, I'll select everything from line four down to 11 and execute the query to see the final result. So now I have the number of products within each oil type. We have the highest price for each oil type, the largest size, the lowest price within each category, as well as the average price within each category. With these three aggregate functions, max, min, and AVG, you can find the highest, lowest, and average value from any column within your groups. Combined with the count, these details can give you a good insight into the range of data that you're working with. It could be useful to know, for instance, if you have a large number of items within each group, but they're all close in value with similar minimum and maximum values. You might want to work with that kind of data differently from data that has very few rows, but there's a large spread between the top and the bottom values.

Evaluate columns with Boolean aggregates

- The Boolean data type is used to represent data that is either true or false, on or off, or yes or no. We can take a look at the customer table to see an example column that uses this data type. I'll simply select all of the columns from sales.customers. Now, in this data table, the newsletter column over on the far right is used to track whether each customer has opted in to receive our promotional emails. Here in pgAdmin, that data is represented by the text true or false. Other graphical interfaces may display this data with checkboxes or other text to indicate the Boolean value, but they all represent the same thing. In this case, Bread Express, Green Gardens and Delish Food have all opted out of the newsletter, indicated by false here in the column. The other three customers have agreed to receive the promotions and their values are set to true. Now, just like our other data, we can use grouping with Boolean columns to get some statistical insight into the data. Let's create a new query here. I'll say select newsletter, and I'll count up the records from sales.customers and I'll group by the newsletter column. When I run this query, we'll see what the results look like. This reveals the exact count of how many customers receive the newsletter and how many don't. So we have three that are opted in and three that are opted out. Now, because we're dealing with simple true/false or yes/no data, it doesn't make any sense to try and apply a min, max or average function to this data. In fact, if you try and do so, you'll get an error. For instance, I'll come back up to line number three and I'll type in a comma, and we'll try and add in a max function applied to the newsletter data. This time when I run the query, you'll see we get an error. We can't apply a max function against a Boolean data type. So you can't use the min, max or average functions against Boolean data but there are a couple of other aggregate functions that you can apply to Boolean columns and they're useful for quickly finding out if all of the values in a column or a group are true or if only some are true. The two functions are called bool_and and bool_or. Let's add them both into a new query and then we'll describe what the results reveal. I'll come down to line number seven, and we'll select the state column. The count of all the records. We'll use the bool_and function against the newsletter column. And I'll use the bool_or function also against the newsletter column. We'll pull all of this from the sales.customers table. And we'll group the data by state. Okay, I'll run lines number seven through nine, and we'll see those results here. So in these results, we're grouping the customers by the state that they're in using the two-letter state abbreviations. There are two customers in Ohio and Georgia, and one customer in Michigan and Iowa. Then we have the two columns that represent the bool_and function and the bool_or function. The bool_and function displays a true or false column that reveals whether every customer in each group is signed up for the newsletter. If every record within the group is true, bool_and also returns true. If a single record within the group is false, then bool_and returns false. Bool_or, on the other hand, is looking for the presence of a single true value within the group. If one record in the group is true, then bool_or returns true. Looking at these results, I can tell that nobody in the state of Ohio is receiving the newsletter since both bool_and and bool_or are both false. The row for Georgia shows mixed results. Bool_and is false while bool_or is true. This means that at least one customer is receiving the newsletter but not everyone in the state is. So using these two aggregate functions on Boolean data can get you to some quick insights about the full dataset, and allow you to see if every row shares the same Boolean status or not.

Find the standard deviation and variance of a dataset

- In statistical analysis, scientific measurements, two additional aggregate functions may be valuable. The standard deviation and variance of a set of data, both describe the spread or the dispersion of values from the norm. Now, these types of statistical computations aren't typically used with retail data. So in order to demonstrate, I want to load in another dataset into the PostgreSQL database. In the chapter one folder of the exercise files, you'll find a file called people_heights.txt. Copy of the text into a new query window like I have here. And you'll see that it creates a table and it inserts a number of data rows into that table. Execute the script. And it'll create that table inside of the public schema. And it'll add 400 measurements of people's heights, accompanied by their name, and biological gender. Now, we have a good set of data values to evaluate with the standard deviation and the variance functions. So I'll go ahead and close this query down. I'm not going to save any changes to it. And we'll start up a new query window. So let's start with the basic statistical calculations or the aggregate functions that we've already seen. We'll select the gender column, count up all of the rows in the table with the count function. We'll use the average min and max functions against the height_inches column. And we'll pull all of this data from the public.people_heights table. And we'll group the values by gender. Let's execute the query to see the results that we're getting right now. So here we have the two biological genders, male and female. We have a count of the number of rows. We have 200 rows in each dataset. Here is the average value. So about 69 inches for males and 64 inches for females. And then we have the overall minimum and maximum for each gender. So this gives us a good idea about the range of values for each gender. And it tells us the highest and lowest height as well as the average. So we know the range of data that we're working with. But what we don't know is the shape of the distribution are all 200 height measurements, uniformly distributed across this range between 61 and 76 inches for males and 57 to 73 inches for females. If you've ever taken a statistics class, then you probably know that an even distribution is unlikely. And that the true shape of the distribution of values should resemble a bell curve. With more values near the average and very few values at the high and low extremes. These statistical tools that we have for understanding the shape of a bell curve or normal distribution are standard deviation abbreviated as std dev or variance. Now, historically, you could use those two names for the functions. But in PostgreSQL, it now uses two different forms of the calculations, depending on your data. Let's add all four into the query. I'll come up here to the very end of the max calculation here at the end. I'll add in a comma. And we'll jump down to line number two where I'll type in the new function, stddev_samp. We'll apply it to the same height_inches column. The next one is stddev_pop. And we'll apply it to the same data. Those are the two standard deviation functions. Now, we also have var_samp and var_pop for variance. Now, when I execute the query, we'll have four additional columns added into the results. We have the standard deviation of the sample, the standard deviation of the population, the variance of the sample, as well as the variance of the population. The two versions that end in _samp are to be used when your measurements consist of a statistically significant subsample. And you want to understand the entire population. The _pop version of each function is to be used when your measurements consist of the entire population that you're studying. As the number of samples increases, or in other words, the more rows that you have in your original dataset, the two functions will converge and report the same result. The standard deviation in particular gives us a good understanding about where the majority of our values occur between the full spread, from maximum to minimum. With a standard deviation of about 2.7. That means that approximately 68% of our height measurements will fall within 2.7 inches above or below the average. So nearly everyone will be within the range of about 66 to 72 inches for males and 61 to 67 inches for females. Following a normal distribution, we can also expect that 95% of our data points will fall within two standard deviations. Or about 5.5 inches above and below the average. So while these types of statistical calculations aren't used in typical business scenarios, standard deviation and variance can be invaluable tools when working with scientific data and measurements.

Include overall aggregates with ROLLUP

- When analyzing data using a spreadsheet application like Excel, it's common to include total rows that sum up all of the values within a group. In the SQL queries that we've looked at so far, the group by a clause has returned statistics for each individual group, but we don't get those same kind of totals across multiple groups. There is a way to do this in a query though with the rollup option. I'll start with a grouping query that gathered some statistics about the two trees, olive oil products. (keyboard clicking) This query will pull the category ID and product name columns from the inventory.products table, and we'll also calculate some columns with the account of all of the rows, the minimum price, the maximum price, and the average price. And then grouping all of the rows based off of the category ID first, followed by a subgroup for each product name, and then we'll order the data based off of the category ID's and then the product name so that everything is in a consistent state. So, let's go ahead and run this query and take a look at the results. This returns 31 groups, one for each product category and oil type or product name. We also get the lowest, highest and average prices of the products within each of these groups. Now, in order to get subtotals and overall totals, all we need to do is add the rollup keyword into the group by clause. On line number eight, I'll come here right after the keywords group by, and I'll add in rollup. And in order to get this to work, we also need to have the group by category ID and product name values wrapped inside a parentheses. So I'll go ahead and add those in as well. Now when I execute the query, you'll see that we get a total of 35 rows now, whereas before we had 31. So, we've gained four new rows in this result set with that small change to the query. Let's take a look at now what this query is showing. At the top we have the data for category number one and all of the oil types that it contains. This part hasn't changed at all. Then I'll scroll down until I get to line number 19. You'll find that this row has a no value for the product name, but it's in category ID number one. This row is a subtotal for all of the products in category one. It shows the category number one has a total of 89 products across all product names. Here is the lowest price of 899 across all of those products. The highest price is 2799 across all of those products, and here is the average price across all of the products in category one. After this, we have the lines that show each product name within category ID number two. See here we have category two, basil infused, extra-virgin oil, chili infused oil, and so on. Let me scroll through the list until I get to line number 26. So, here is a subtotal for category ID number two. It says no here for the product name. In category number two we have a total of 18 products across all product names. There is the lowest price, the highest price and the average price for all of the products in category two. Then we have category three and the breakout for the individual products there. And I'll scroll down to the bottom where I have a subtotal for all of the products in category three on line number 34. It has a total of seven products and we have the lowest, highest and average price for that. Finally, at the very end of the results, we have a grand total row for the entire dataset. Across all categories and product names there are a total of 114 products. The lowest price product across the entire inventory is 699, the highest price product is 2799, and the average of all products is 1759. So, without the rollup keyword you'd have to write out separate queries with different grouping levels to get to these kinds of summary calculations. With the rollup keyword added to your group by clause though, you can include these summary rows, the display subtotals, and grand totals for each of the groups. This allows you to get to this useful information with a single query, which can speed up reporting tasks and have less of an impact on database operations.

Return all possible combinations of groups with CUBE

- We saw that the ROLLUP keyword, create subtotal and grand total rows in your Group By clauses. And this is suitable for hierarchically organized information where you are, specifically, looking at sub-categories within larger parent categories of items. There's another keyword that we can use with Group By that'll return all possible combinations of groups when you're using multiple Group By columns. This is useful when your data can be segmented into different groups, but those groups don't necessarily have a relationship to one another. Let's look at an example to get a feel for when this may be useful. I'm going to start with a query that pulls the category_id and size columns for our products, and then calculates the count, min, max and average prices. Once again, I'm going to use the ROLLUP keyword to get the subtotal and grand total rows in the results. When I execute the query this returns 16 rows, and we can see that we have the breakdown of each product category. So I have, here, product category one, and I'll scroll down to find product category two and three, and then, at the very bottom, we have the subtotals for each category, here on line 15 for category three, and line 10 for category two as well as a grand total for the entire dataset on line 16. So our products come in different categories and have different sizes, but those two attributes aren't actually related to one another, they're just different ways to describe each product. Notice that we have products of the same size spread across the different categories. For instance, there are 16 ounce products in category one, two and three. In fact, if we take a look at the actual data, I'll scroll back up here to the top, here are the 16 ounce products in category number one, and we have 17 of those. In category number two we also have some 16 ounce products, and there are six of em there, and in category number three we have 16 ounce product, in fact, it was just the single one there. So what if you also wanted to see subtotals for these products by size, without regard for the different categories that they're in. This is where the CUBE variation of the Group By clause comes into play. If you come up to the top, on line number eight, and change the ROLLUP keyword to CUBE instead we'll see the difference. So now I'm going to Group By CUBE category_id and size. I'll execute the query, and, this time, we get a total of 24 rows. We still get the same rows for each category and the subtotals for each category. Let's go ahead and scroll down through the list. At the bottom, though, you'll find subtotal groups for each of the sizes broken out by themselves. This is without any consideration for the different categories that they're in. Take a look at the row, for these 16 ounce products, on line 20. Here, you'll find that it reports 24 total products in the 16 ounce size. That's the 17 from category one, six from category two and the single product from category three. All of them are reported together as their own subtotal row here. So, effectively, what the Group By CUBE clause is doing is giving us four separate queries all at once. It returns all possible combinations of the groups in a single result. This result with CUBE is the same as grouping all of the product data just by the category, just by the size, a combination of size and category, and in a single group for the grand total row.

Segmenting groups with aggregate filters

- Grouping rows of data based off of attributes is a useful technique, but sometimes you want to get even finer grain control over the way that your information is segmented. You can create your own dynamic, partial aggregates by adding in additional filters and criteria. Let's start with a quick look at our product data grouped by category. I'll select the category ID column from the inventory.products table and we'll calculate the total count of all products, as well as the average price of our products. Let's run the query, and this gives me back some basic statistics about the data. Here, we have our three categories. We have a count of how many products are in each category, as well as that average price. Now, we know that our products come in a wide range of different sizes, from very small four-ounce bath and beauty products all the way up to very large, 128-ounce jugs of olive oil. If we wanted to create our own customized segments based off of product size, we can do that with a filter. I'd like to break my products down into two different size classifications. Small products are all 16 ounces or less in size, and large products, which are over 16 ounces. We can build that into the query like this. I'll come back up here to the end of the average price calculation on line number three. I'll type in a comma and come down to line four. First, I'm going to add in a comment just so I can remember that the next two calculations are for these small product sizes. For the first aggregate calculation, I want to count up all of our small products. We can do that with a count function, just like before. So I'll type in count and then parentheses and asterisk. This time, though, I want to add in a filter and use a where clause to define the criteria. I'll type the keyword Filter and in parentheses, where size is less than or equal to 16. This'll make sure that the counting function only counts up the products where the size is less than 16, or what fits into our small size classification. Let's give this column an alias. I'll call it As Count Small. We can do the same thing for the average price calculation of all the small size products. Once again, I'll use the average function against the price data. We'll apply a filter with a where criteria that says the size is less than or equal to 16. I'll give this an alias of Average Price Small. Next, let's fill in the details for our large products. Once again, we'll count up all of the large products. The where condition for this filter will be where the size is greater than 16, and I'll give this an alias of As Count Large. Finally, we'll do one more average price calculation. This will also have a filter where the size is greater than 16, and I'll give it an alias Average Price Large. We'll still pull the data from the inventory.products table. We'll group by the category ID and we'll order the results based off of the category ID column. Now, I just noticed that I missed the comma up here at my average price calculation for the small products, so I'll come back up and add in the comma there, and then I can run the query. The results add in the additional columns with the full dataset segmented into those two size classifications. For category number one, there's a total of 89 products, and here is the average price. Of those 89 products, 35 fall into the small category, and there's the average price for all of those small products. 54 of the 89 products are in the large category, and here's the average price for the large products. So by adding in the filters up into our aggregate calculations, we can break our full data set down into smaller segments. We can even add in the roll-up keyword into the group by clause, by typing it in here on line number 11. Then, when I execute the query again, it'll give me another final group total at the very bottom. This reveals the overall totals for the entire dataset, and it shows me the total breakdown of all of my small products. In this case, we have 54 small products across all three categories, and we have 60 large products across all three categories. So with filters added into your aggregate functions, you can create your own customized data classifications. Getting creative with your use of where conditions within those filters will allow you to segment your results into a very fine-grained level of detail.

Challenge: Group statistics

- We've seen a number of ways that you can incorporate grouping into your Postgres SQL queries. Let's test what you've learned now with a challenge. I'd like you to write out two queries that dive into the order data for the Two Trees Olive Oil Company. For the first query, explore the sales dot orders table to find the number of orders placed per month for each customer. To segment out the data, use a filter, and remember that date values need to be placed within single quotation marks. For the second query, explore the sales dot order lines table. Write a query that returns the quantity of each product sold. To accomplish this task, you'll need to use an aggregate function named sum. The sum function works exactly like the min, max, and average functions that we looked at earlier in this chapter, I estimate that this challenge should take 10 minutes or less to write out the two required queries. In the next movie, I'll walk through my solution to the challenge, good luck.

Solution: Group statistics

- I hope you were able to write out the two queries to explore the Two Trees sales data. Let's take a look at how I'd approach the solution. I'm going to start a new query tool here inside of pgAdmin and we'll just write out a note here using a comment to remember what it is that we're trying to accomplish. For this particular query, we're going to be relying on the sales data coming out of the orders table in the sales schema. So I'm going to start just by taking a look at that table all by itself, just so I can see the raw data that we have to work with. So we'll say select star from sales dot orders, and I'll run that query here. Okay, so this table has three columns, order ID, order date and customer ID. Now for this challenge, we want to find the number of orders. So that's going to tell me I'm going to count up all of these rows. I want to find the number of orders per month. So the per month tells me that I need to create a segment based off of the order date and then for each customer. So I'm going to be grouping the data based off of the customer values. Now let's focus here on the per month section that we need to answer here. If I take a look at the order dates, I have the order dates of, here's March 15th, March 17th, March 19th, 2021. So these are all in March. Let me scroll through here. So we have lots of orders in March, okay, and so here's the end of March, the beginning of April. So we have April 1st, 2021 and if I scroll through the rest of the dataset, we'll see that they're all in April. So we really only have two months that we need to focus on here for my segmentation or the filtering based off of the order date. Okay, with all of that in mind, we can go ahead and get started writing out the query. So, I want to select the customer ID since that's the column that we're going to be grouping on. So I'll select the customer ID. Then, I also need to find the filter of the order dates. So I'm going to be counting up the number of orders for each month. So for that, we're going to use a count function, we'll count star, and we're going to need two filters, one for March and one for April. So, go ahead and filter for March 1st. Say where, the order date. Now in order to specify a range of dates, we can use greater than and less than. So I can say for instance, where the order date is greater than or equal to, and in single quotation marks, remember this is a date, so we need single quotes, so 2021, 03, 01 and order date is less than or equal to whatever the end of March is, so 2021 dash 03 dash 31. So that'll specify the full range of dates that are in March. So where the order date is greater than the 1st of March and the order date is less than the 31st of March. So that'll filter out the first set of dates. Then we need to figure out the dates for April. So we'll do the same thing here, we'll count star, filter, where, order date. Now another way that we can do this inside of PostgreSQL, instead of using the greater than or equal to and less than and equal to syntax, is to use 'between and'. So just to mix this up, I could say between 2021 dash 04, 01 and 2021 dash 04, and there's only 30 days in April, so I'll say dash 30. That's another way that we could specify the date ranges using the 'between and' syntax. So either way, gets the exact same results. Okay, so we're selecting the customer ID, we're selecting the dates in March, we're selecting, or counting the dates in March, and we're counting the dates in April. Then we can go back here and specify where all of this data is coming from. From sales dot orders and I want to group by the customer ID. All right, let's go ahead and select all of this here and execute it and there is the result. So here are the customer IDs and it tells me that I have, well, this first column corresponded to our March dates and the second column corresponded to our April dates. So I have a count of how many orders are placed for each of our customers in each of those months. Now just to clarify this, I'm going to come over here to the very end. We'll give it some better aliases here. We'll call this "as March" and we'll say "as April". All right, let's execute this again. There we go, there are some better aliases here at the column headers at the top to clarify what each of these are. So that answers the first query. We come down a couple of lines and we'll start on the second part of the challenge. Once again, I'll type in a comment here to remind myself what it is that we're trying to accomplish. So for the second part, we wanted to find the quantity of each product sold from the order line of this table over here. So let's select everything out of that, just to see what we have to work with. We'll select everything from sales dot order lines, highlight that and run it. So here's the raw data that we have to work with. So I've got the order ID column, line ID, I've got the sku, so this is the specific products that were ordered as well as the quantity of each product for the order line. Now, because these quantities aren't all one, that's the reason why we need to use a sum function instead of a count function for this. If we just say count, it would only count up the row for each product, where I wanted to find out the number of total products sold, which in case we need to take into consideration that some lines include multiples of the same product. So here we have two of have whatever this oil is, two of this oil. On this line, it only sold one of that oil and so on. If you can scroll through here and see that some of those quantities are higher for different products. Okay, so let's go ahead and start writing out the query for this. I'll come down here. In fact, let me just, okay, so we're going to select the sku column and I want to sum up the quantity column and this is just going to be from the sales dot order lines table. We want to group by the sku and let's go ahead and just run it right now. Let me highlight all of this and execute it. All right, so there are the skus and there is the sum total of the number of products sold for each of those individual product skus. We can scroll through and see all those numbers there. I can make this a little bit better by adding in a sort. So we'll say order by, let's say the sum of quantity, descending, DESC, in which case, this will put it in a descending order, so we could see the products that were sold the most here at the top, so this oil here FCP128, we have 22 sold of this oil PIC016 and another thing we can do is say group by roll-up and if I do that and then run it, that'll give me a sum total of all products sold. So we fold a total of 478 total products and then here is the breakdown for each specific sku below. So that is my solution to the second query. So, I'm going to go ahead and save all of this in a file called challenge one complete dot txt inside of the exercise files so that you can compare my solution against yours.

USE WINDOW FUNCTIONS

Create a window function with an OVER clause

- Aggregate functions can reveal useful statistical information about your dataset. We've seen how to use them with row grouping to calculate min, max, average, standard deviation and sums across multiple rows that all share a particular attribute. These aggregate functions can also be used without grouping using a technique called a Window function. This allows you to report these calculated values, right alongside each row of data that they correspond with. Let's take a look by starting with a grouping query. This query will select the category ID column as well as the average price for all of our products. When I execute the query, I get these three rows. This reveals the average price for all products within each of our three categories. Now, take a look at these calculated values for the average, we have 18.5, 13.4 and 14.6. Now, if we remove the category ID column from the query and just select the average price of the inventory.products data, if I rerun the query, we get just that column but you can see the calculations are exactly the same 18.5, 13.4 and 14.6 the presence or absence of the category ID column and the results has no effect on these calculated values. Since it's the group by clause, that's the element that's determining how to separate the different prices into our three categories. Now, if we remove the group by clause entirely, and just say select average price from inventory.products and execute the query again, we get a different calculation. This is now telling us the average price across all of the two trees products, and that comes out at 17.59. Now it may be useful to be able to reference this OVERall average price value along with the details about each individual product. This is where the Window function comes into play. Window functions use the OVER clause, in order to turn a regular aggregate function into a Window function. Watch what happens when I come up here into my query and add OVER, open and close parentheses after the average calculation. Now, when I execute the query, we get a number of different rows, but the calculated value is exactly the same we get 17.59 again, but this time it's displaying an entire column of the same value. In fact, there are 114 rows in this result set. If you recall, 114 is exactly the number of products that we have in this table. So the Window function is allowing us to calculate the overall average for the entire dataset, but it's doing it in a way that doesn't group all of our data rows together. At this point we can add in the other columns from our products table that we want to see right alongside this OVERall average price. I'll come back up into my query and I'll give myself some more room and we'll fill in a couple of additional columns. So I'll select the SKU column, the product name column, the size column and the original price column. Then I'm going to bring this back up and we'll just tab this over to make everything line up. Now when I execute the query, you'll get this result, we have a single row for each product, we can see all of its details here including its unique price. We also have the overall average price for all products reported on each row. This makes it easy to evaluate whether each product's price is above or below the average and by how much. The OVER clause turns regular aggregate functions into window functions by creating what's called a window frame. The window frame is a set of data rows that the aggregate will operate on. In our case by using a empty set of parentheses in the OVER clause we specified a window frame that includes all rows in the original data set.

Partition rows within a window

- We just saw that the over clause will turn an aggregate function into a window function by operating on a set of data rows without actually grouping them together in the final result. With an empty set of parentheses, the over clause creates a window frame that includes all rows from a dataset. We can create partitions within this window frame in order to control the set of rows that the aggregate calculation will operate on. Let's first take a look at the average prices split up by product size. This query uses a basic aggregate function and grouping to display the results. The rows revealed the average price for each product size. I want to focus on a single size, the eight-ounce products. The average price for all eight-ounce products is $10.53. Keep that number in mind, and we'll come back to it in just a moment. Now with a window function, we saw that we can display the overall average price across an entire dataset with the over clause and an empty set of parentheses. In the prior movie, we wrote out a query that looks something like this. We'll select the sku, product name, size, category_id and price columns from the inventory.products table, and we'll also calculate the average price for the entire data set using an over clause with an open set of parentheses. When I run just this query here, we'll see these results. So here is the average price for all of our products. And we saw that it was $17.59. The empty parentheses is why the average is being calculated for all products. You can probably see where this is going. By filling in some additional details, we can calculate an average over a specific subset of our products. We do this by creating partitions within the window frame. In order to do that, we're going to come up into the over clause, and inside of those open parentheses, we're going to type partition by size. Let's also give this column an alias. I'll name it average price for size. Now when I run this query, I'll just highlight everything and execute it again. We'll see a different result in this column. Now, instead of calculating the average price for the entire data set, the average is being calculated for each unique size classification, and the number that's reported corresponds to the classification that each product is a member of. Find any eight-ounce product in the results. For instance, this first line here for the delicate oil in the eight-ounce size. We can see its price is $10.99, but the average price for that size classification is $10.53. This number is repeated for every eight-ounce product that's in the results. For instance, here's another one. The basil infused oil at eight ounces has an average price of 10.53 again. The partitioned window function calculates a value in this column that's different for each product size group. We can now finish up this query by calculating the difference between the product price and the average price for its size. I'll come up into my query and I'll add in one more row. This time we'll take the original price. We'll subtract from it the average price over our partition by size. I'll also give this column an alias. I'll just call it difference. Now I can highlight the query again. Let me scroll down a little bit and select rows 6 down through 14. I'll execute it. That gives us one additional column here at the very end. That gives me one additional column that gives me the difference in price from the original price of the product and the average price for its size classification. Products that are priced above average for their size will have a positive number here in this column and products that are below average price will have a negative number. With a couple of partitioned window functions, we can quickly determine how far above and below average each product is priced when compared to the other products in the same size classification

Streamline partition queries with a WINDOW clause

- If you write a query that includes multiple window functions, it's not uncommon to use the same partition over and over again. For instance, you may want to review the average, minimum and maximum price for each product within the same size classification. Using the OVER clause and a window frame partition, it might look like this. We'll pull the sku, product name, category ID, size and price columns from inventory.products, and we'll calculate the average min and max price over a partition by the product size. When I execute the query, the results give you these statistical details about the size group that each product is a member of. But there's a lot of repetition with the same over clause written out several times. If you wanted to alter the query, to look at a different partition, you'd have to update all three lines. This process can be simplified by specifying the window a single time within your query. Let's create a new line between the from clause and the order clause here. Here, we'll enter in a new window clause. The purpose of this is to define our window partition. We start by giving it a name. It can be whatever you want. I'll call mine xyz. Then the as keyword, and in parentheses, whatever you want the partition to be. We'll use the same common partition, partition by size. Once the window partition is named and defined, we can go back through the query and reference it by its name. So here for the average price calculation over our partition, I'll just use the name instead, xyz. We'll do the same thing for line seven and eight. Now, when I run the query, the end result is exactly the same. All of the data rows returned and the calculations haven't changed at all. The difference is that the query is a little bit more streamlined and easier to edit. Now, if we wanted to partition the data by category ID, instead of size, we only need to make that modification within the window clause instead of three times in each OVER clause. So let's change this partition to be category ID. Now, when I execute the query, the calculations are now displaying the average min and max for every product within the same category as each row. With a window clause, you can name and reuse the same partition over and over again within your window functions.

Ordering data within a partition

- The window frame created by an over clause contains the set of rows that a window function will operate on. We've seen that we can create dynamic partitions within this window frame, but we can also sort the rows within the frame. This sorting can have an effect on the way the aggregate function is applied. To demonstrate how this works, I'm going to query the inventory.categories table just to get a couple of numbers to play with. We'll simply select the category_ID column from the categories table. The results give me a single column with the values one, two, and three. Now we can calculate and add up these three numbers by using a sum total with an over clause. Let's add that into the query. Remember that if you leave the over clause empty, we have an open set of parentheses here, then the window frame will contain all of the rows. When I execute the query, we get a second column here that has the value of six for every row. This is the sum total of one plus two plus three. However if we come back into the over clause and add in an order by clause into the window frame, watch what happens. We'll say order by category_ID. Now when I execute the query, we get a different result. Now the sum function is only operating on rows up to and including the current row. So for the first row we get the number one. For the second row we get three which is one plus two and for the third row we get six, one plus two plus three. Effectively what we've done is turn the sum calculation into a running total instead of a sum total. Now to see how this can be useful, let's apply it to our sales data. I'm going to get rid of this query and we'll type out a new one. This query combines information about our orders and the products pricing. Let me go ahead and execute the query and we'll take a look at the results. Oh, looks like I have another typo here. We have a table named products that I misspelled. Let's go ahead and fix that and re-execute. All right, there's our results. So here we have on the left we have the different order ID numbers. Each order can be made up of one or more lines. Order ID number 100 for example only has a single line for this oil. The person ordered to have that item at 15.99 for a line total of 31.98. order number 101 was made up of four different lines. one, two, three, and four. Here we have the four different oils that were ordered, the quantity of each item, the original price each, as well as a line total that multiplies the price each by the quantity ordered. Now when looking at sales data like this, you might also want to know the total price for the entire order, as well as the running total as each line is added. Using window functions, we can get that information. Let's go back up into our query and I'll type a comma here at the end of line number six, and we'll come down to the next line. I'll start with another sum function. We're going to add up the line total which is made up of the quantity multiplied by the price each. Then we'll use an over clause to specify a partition. When we partition based off of the order ID, that'll give us a grand total for the entire order. Let's add in another some function. Once again, we'll sum up the quantity times the price. We'll add in another over clause. We'll use the same partition as before, but this time we're going to add in an order by clause. We'll order them by the line ID. This will give us a running total for each order. Now we'll execute the query and we'll see those two additional columns. Let's take a look at order 101 again. The first row is for this oil. We have two items ordered at 26.99 for a line total of 53.98 and our running total is now set to 53.98. The second line on the orders for a different oil. We had one of that item at 18.99 for a line total of 18.99. When you add this to the previous line, we get a running total of 72.97. Then we add in another product at 10.99. That brings us to a running total of 83.96. And finally we add in one more product at 14.99 for a final running total of 98.95. That brings the order total to 98.95 which is reported in all four of these cells. So by ordering the data within the window frame, you can slightly alter how the aggregate functions operate. With order data, calculations only include rows up to and including the current row instead of all rows defined within the window.

Calculate a moving average with a sliding window

- When the window frame contains sorted data, you can add an additional parameter to create a dynamically changing set of records that an aggregate function will apply to. This technique allows you to create moving averages and rolling sums of your data. First, let's gather a few numbers that we can experiment with. I'll simply use the numbers that can be found in the order_id column of the orders table. That returns a single column with a bunch of numbers. Now, a moving or a rolling calculation applied to this data will incorporate different rows as it moves down this column. For instance, a three-period leading rolling sum of the first cell would add 100, 101 and 102. The same calculation of the second cell would add 101, 102 and 103. As each row is processed the rolling or moving calculation adjusts to incorporate the immediately adjacent rows. To add these kinds of calculations into your query, we start with the same window function and over clause. Let's add a rolling sum up on line number two. We'll start with the window function. sum of order_id, over and open a parentheses. Next we need to specify the ordering or the sort. I'll sort my data based off the order_id column. Next, we can specify the period, or the specific rows that we want to incorporate into our rolling sum. We do that with rows between, and then a number proceeding and a number following. For a three-period leading sum, we'll say rows between 0 proceeding and 2 following. I'm going to give this column an alias as three-period leading sum. Now it's important to note that the window function will always include the current row and we're now also incorporating the two immediately following rows. That brings us to a total of three units, or what's typically referred to as a period in these types of calculations. When I execute the query, we'll see that additional column here. So our first value 303 is the leading sum of 100, 101, and 102. Next we see 306, which is the sum total of 101, 102 and 103. So that's what a leading summation would look like. We can change this around and change it to a trailing summation. Let me copy lines two and three to my clipboard. Then I'll type in a comma here at the end and paste those cells in. Then we'll change this function, so it'll say sum order_id over order by order_id rows between, and I'll change this to 2 proceeding and 0 following. That'll change this into a three-period trailing sum. When I execute the query, we'll get this third column here. The value of the first cell is the sum of 100 plus the two prior cells, which don't exist, so that's why we just see 100. Here we see 201, which is the sum of 101 plus 100. And then we get 303, which is 102, 101, and 100 added all together. So that's what a trailing sum would look like. Now, this type of dynamic calculation is most often used to create a moving average. Let's take a look at what that would look like. I'll type in another comma, come down to line number six and paste in our copy once again. Then I'll change the function to AVG for average. For the rows between I'll change it to one proceeding and one following, and that'll create a three-period moving average. This average will incorporate the previous row, the current row, and the next row. A moving average is useful because it can smooth out some of the noise present in highly variable data. And it's something that you'll encounter a lot in graphs and charts in order to help visualize underlying trends through that variability. You'll also encounter trailing averages that only incorporate past measurements in the calculation. Let me go ahead and execute this query and we'll see what that column looks like. So each of these cells is the average of the previous cell, the current cell and the following cell. So by limiting your window frames to a specific number of rows proceeding and following the current row, you can create these kinds of moving and rolling calculations in your analysis projects.

Return values at specific locations within a window

- There are a couple of other window functions that you can apply to data within a frame once it's partitioned and sorted. You can retrieve a row's value based off of its position within the frame using the first, last, and Nth value window functions. To explore how this works, let's review the data about the two tree's customers. I'll just grab all of the columns from the customer's table. Now in this database, we have information on six different customers. I'm going to be focusing on just the company column. We can take this column of values and place the names within a window frame. Then, selectively retrieve any name from the list. To start the process, I'll come back up into my query and I'll modify it to select the company column. This will retrieve that column from the data table and display it just like it exists in the original table. For the next column in the query, use the first_value function. Then, we can create a window with the company names. In the over clause, we want to make sure that we're sorting the names in alphabetical order. Otherwise, these window functions might have unexpected and unpredictable results. So inside of the parentheses, type order by company. As you might expect, the first value function should retrieve the first name from the list. In our dataset, that should be blue vine. Next, we can use the last value function in exactly the same way. I'll create another window for the company data and it'll use these same over clause. You would expect this function to retrieve the last alphabetically listed name, and in our dataset, that would be wild rose. Finally, there's an Nth value function that'll allow us to choose any value from the window frame by its position. There's one additional requirement for this function. I'll type in a comma, and I want to retrieve the third value in our list, so I'll type in the number three. Looking at the data, that should be delish food. I'll close the parentheses and add in the final over clause. Okay, that finishes our three window functions. Let's execute the query and take a look at the results. So as you can see, it sort of worked, but not entirely. The first value function did indeed return blue vine for all of the rows. The last value column is only returning the same name as in our first company column over here on the far left. The Nth value function is partially working and showing delish food for most of the rows, but the first two are null. So, why might this differ from our expected results? To answer this question, you need to recall that within order by sort in the over clause, the window function only works with values up to the current row. So that's why the last value calculation is only retrieving the same name and why the Nth value is only working once we get to the third row. To fix this, we need to expand the range of rows that the function is operating against. You might think that we can just remove the order by clause, and that may work sometimes, but it's not really a good idea since our query requires that the data is sorted alphabetically. Instead, the better option is to use the rows between syntax. Instead of specifying a specific number of rows preceding and following, we can add a bit of text like this. This is going to get a little long, so I'll place my cursor here after the word company and before this closing parentheses on line number two, and give myself some more room to type. I'll start with the rows between keywords. Now, instead of specifying a specific number, I can say unbounded. So we'll say rows between unbounded preceding and unbounded following. This tells the window function to take into consideration all rows within the frame, instead of just the rows from the beginning of the frame up to the current row. So let me copy this text that I have here on line number three, and I'll copy that to my clipboard, and I'll do that same thing with the other two functions. Now, when I execute the query, I get the expected results. Blue vine is the first value in our window. The last value is wild rose, and the third value, or the Nth value, is delish food. So that's how these location-specific window functions work. Let's take a look at a real-world example now. I'm going to turn our attention to the order data and come down here onto line number 11. And in fact, let's come down a little bit more and give myself some more room to type. First, I'll select everything from sales.orders. I'll highlight line number 11 and execute it to take a look at our raw data. Now, our six customers have placed a number of orders. It would be useful to be able to know when they placed their first order and when they placed their most recent order. To do this, we can pull the customer IDs from the table. I'll start a new query that selects the customer ID column. Then, we can choose the first value function and I'll pull the order dates. Now, this is going to get a bit long again, so I'm going to break this onto multiple lines. Now, we don't want all of the order dates included. We only want the dates for each specific customer, so we need to create a partition by the customer ID. So inside of our over clause, I'll say partition by customer ID. Next, we want the window frame to be sorted chronologically. So, add a sort on order date. Then, we need to expand the window frame to include all dates and not stop at the date corresponding to the current row in the table. That'll require the rows between unbounded preceding and unbounded following text. So that finishes the first value function. Type in a closing parentheses and a comma, and come down to the next column in the query. Now, all of this is going to be exactly the same, we're just going to change the name of the function. So I'm going to highlight all of this and copy it to my clipboard. Come down onto line number 18 and paste it in, and then change the function name to last value. Finally, I'll come to the end. I'll get rid of that trailing comma at the end. We're going to specify the table where the data can be found. That'll be from the sales.orders table, and I'll add a sort to the final result. We'll order all the rows by the customer ID. Okay, let's go ahead and highlight lines 13 down to 23 and execute. The results show 61 rows, which correspond to our full list of 61 orders. But you can see that most of the rows are identical. Remember that window functions don't have any effect on the number of rows returned. And since we're ultimately querying the sales.orders table, we'll get the same number of rows back that that table contains. Since most of the rows are the same, though, we can come back up here into our select clause and add the distinct keyword. Then, I'll execute the query again, and we'll see our final result. This displays only the unique values. Here is the information on our six customers, along with their earliest and most recent order dates. So that's how you can use window functions to locate specific values within your window frames.

Challenge: Leverage window functions

- We've seen a number of ways that you can use window functions to get more information out of your data. Let's test what you've learned with another challenge. In the inventory.products table you'll find a list of all of the products that are sold by Two Trees. I'd like you to write out a single query that uses window functions to expand on the information about each product. Along with other product details, return the min, max, and average price, as well as the count of products that share the same category and size classifications. Remember that you can include a window clause when you have multiple functions that all use the same window frame parameters. I estimate that this challenge should take about five minutes or less to complete. In the next movie, I'll walk through my solution. Good luck!

Solution: Leverage window functions

- I hope you were able to use window functions to give additional context to the product data. Let's walk through the solution that I came up with. I'm going to start up a new query window here inside of PG admin. And as always, I just like to start with the raw data. So I'm going to select everything from inventory.products, and we'll take a look at what we have to start with. So this table has these columns, skew, product name, category ID, size, and price. So that is the raw data out of the table that I'm going to start with. And then I also want to append additional columns. That'll give me the maximum, minimum, average, and count of the pricing information over the category and size classifications. Now, the reason I want to partition based off of category ID and size is so that I can compare products within the kind of similar groups. So for instance, I don't want to compare an eight ounce olive oil that occurs in category one against an eight ounce, say bath and beauty product that occurs inside of category number three. And actually it doesn't look like we have any eight ounce products in that category, but you get the idea. You want to make sure that you're comparing against similar categories and size groups. So that is what we're trying to do. Let's go ahead and start writing out our query. We'll just continue working on this one. So instead of selecting everything, I'm just going to write out the different columns that we want to pull. So let's say category ID first and then the product name. And I'm not going to pull the SKU in my final results. So category ID, product name, let's pull the size and price as well. Okay, so that's my initial columns. We're going to pull it from the inventory.products table. Now I also need to do the calculations. So you want to find the max of our price, and we also want min of price, and we want average of price, and we want a count of everything. So I'll say count star. Now we need to take a look at our window function. So we know all of these are going to have an over clause. So I'll just go ahead and put that in here. In fact, I'll just copy this and paste that in to all of these, they're all going to be window functions there. And so now we just need to figure out the partition. And since all of these are going to use the same partition, we might as well use a window clause. So after the from clause I could type in a window clause and let's just call it window. Let's just name it W. So I'll say window W as, and in parentheses partition by, and I want to partition it based off of the category ID and the size column. So we just write that out, category ID, and comma size. So that names our partition. We're naming it W. And we're defining it as partition by category ID comma size. So I'll just go back up here to the over clause and I'll type in a W for each of these. And that'll reference the partition here so I don't have to type it over and over again. And that I think finishes it off. Let me add some commas here to separate all of these different columns. And just for good measure I'll also add in a sort to the final result. So let's say order by category ID, and we'll sort it by the product name as well. That'll just keep all of the products in sequence and keep all of the products with the same name next to each other in the final result. All right, let's go ahead and test it out. And there's the results. So there's category, number one here at the top. We have our bold product at the top, the different sizes. In fact, let me put these in size order as well just so that they're in numeric order. So add that into my order by clause, run it by size. All right. So now our sizes are in sequence as well. So here's our size for the bold oil in category one, eight through 128. There is the price of each product. And then we have the max price for all of the eight ounce products is 1199, so it's the same here. Then we have the minimum price is 899 for the eight ounce products, the average price, which is getting cut off, so there we go, average price 1049 for all of the eight ounce products in category one. And there are a total of 18 eight ounce products in category one. And we can go through and compare that. So every time we have an eight ounce product in category one, it should be the same max, min, average, and count. So let's take a look. Here we have another eight ounce product there, and it in fact is the same max, min, average, and count. So that is the solution that I came up with. As always though, there's multiple ways that you could get to the same solution. So you might have come up with a different query. I'm going to save this as challenge two complete.TXT in order that you can compare your solution against mine.

STATISTICS ON SORTED DATA

Calculate the median value of a dataset

- In statistics, there are multiple ways to evaluate a dataset and determine the average value or what's more specifically called the central tendency. You may know these by the terms mean, median, mode, and range. Each one describes the central value in the set in a slightly different way and each one can be useful in different types of analyses. Now we've already talked about the mean of a dataset. That's the traditional average where you add up all of the values and then divide by the number of data points. PostgreSQL handles this with the AVG function that we've used with groups and window functions. So this chapter is going to focus on the other three measurements of central tendency starting with median. The median of a dataset can be found by sorting all of the numeric values from low to high and then finding the value that occurs at the midpoint of the list. To explore this, let's return to the height data that we looked at back in chapter one that we added into the public schema. If you didn't load this data into that schema earlier on in the course, then you can run the script called people_heights.txt that you'll find in the exercise files. First, let's take a look at the raw data. I'll just select everything from the people_heights table. Now this dataset has 400 height measurements along with the biological gender of the person. To find the median height manually, we would need to sort the height_inches column. So let's go ahead and do that. I'll change my select statement to select height_inches. Then we'll come to the end and I'll add in a sort. Now those height values are numerically sorted and we can find the central or the median value. Now since there is an even number of data points here, there technically isn't a value at the exact center of this list. The rows that split the dataset into an equal number of values above and below would be on rows 200 and 201, so let me scroll through the list until I get to those two row. For datasets with an even number of measurements, there are two ways to handle this. One way is called a discreet median and that's to find to the first number in the middle. For our dataset, that will be 66.74 on row 200. The other way that we can find the median is to get the average of the two middle values called the continuous median. Now in our height data, it's just a coincidence that rows 200 and 201 have the same number. So when you average both of these together, you're still going to get 66.74. But a continuous median would technically be the average of these two middle values when the set has an even number of data points like ours does. So now that we've located the median manually and know what number we're looking for, we can see how to calculate it using a function. Let's go back up into the query and I'll get rid of the height_inches column. I'll give myself some more room right below the select statement and I'll also get rid of the order by clause. The function that we need to use is called percentile_D-I-S-C which will find the discreet median. In parentheses, we need 0.5. This is a percentage that tells the function that we're looking for the value 50% of the way through the set. Then we're processing a group of numbers and we need to make sure that they're sorted. The syntax for this is within group and in parentheses order by, the column we want to sort on, height_inches. Finally, I'll give this column an alias and we'll call it discreet median. There's also a function to calculate the continuous median that averages the two central values together for even-numbered sets, and its syntax is exactly the same. It just uses the function called percentile_cont or C-O-N-T. So once again inside our parentheses, we'll type 0.5. Then within group, followed by order by height_inches, and we'll give this an alias of continuous median. When you execute the query, you should get the same values that we found manually. 66.74 for both the discrete and the continuous medians. Now the within group keywords here in the query should give you a hint that we're working with grouping sets for these functions. This means that we can apply what we've seen earlier with grouping our data into segments in order to find values based off of other attributes. Since we didn't include a group by clause in this query though, it's currently processing the entire dataset. However we can segment the height data by biological gender with a few modifications to the query. Up here in the select clause, we'll add in the gender column. Then at the very end of the query, we'll add group by gender. The two percentile calculations will be exactly the same as they were before and I can execute the query again. Now the query takes all of the height measurements, sorts them into two separate buckets for each gender, and then calculates the median value for each group separately. This returns both the discreet median and the continuous median for each gender. We can even add in the rollup keyword into the group by clause and return the overall median. Let me come back up here and right before the word gender I'll add rollup. Now remember that when you do this, you also need to place the grouping column list within parentheses. So I'll wrap the word gender in parentheses there and re-execute the query. This adds one additional row to the results that gives us the overall median, discrete and continuous, for the entire dataset and gets us back to that 66.74 value for all of the data. So the two percentile functions are useful for determining either the discrete or continuous median of a group dataset.

Calculate the first and third quartiles of a dataset

- The percentile function that we used to calculate the median value of a dataset can be also used to find other division points. For instance, a statistical analysis may require that you split the data into four groups called quartiles. The value where a data point transitions between the first, second, third, and fourth quartiles can be determined using the same percentile functions. Let's take another look at our height data. And because our dataset has an even number of rows, I'm going to be sticking with the continuous percentile functions. So I'll select the percentile_cont, and inside of the parentheses we previously used 0.5 to denote the value 50% of the way through the sorted list of values. But we can put whatever percentage we want in here. To find the break point where a row would transition from the first to second quartile, put 0.25. Then we can finish out the function. Within group we'll order by height inches, and I'll give the column in alias as first quartile. The midpoint of a dataset or 50% determines when the value transitions from the second to third quartile. This is also the median that we saw previously so I'll just type in that function. (keyboard clicking) Finally, the transition from third to fourth quartile happens at 75%. (keyboard clicking) That finishes the three calculations that we need to do, and we're finally going to pull everything from the public.peopleheights table. I'll execute the query and the results will show us exactly where the break points are when we need to split the data into quartiles. Values above 63.7275 move from first to second quartile. Higher than 66.74 transitions into third quartile, and above 69.56 would fall into the fourth quartile for our height data. So, that's how you can use the percentile cont and the percentile dist functions to calculate the values where data could be split into quartiles. Now, of course by using different percentages here in the parentheses, you can also find the break point values for any number of segments. For instance, you could use 10% calculations to split the data into 10 parts called a decile. Now, there is another function that seems like it would be helpful for seeing quartiles along with the original measurements. The ntile function is a window function, and it can be used to break the data into groups. However, there's a big problem with using it to determine statistical quartiles that you need to be aware of. To see what the issue is, let's write out the query and then explore the results. I'll come down to line number seven and we'll start another select query. (keyboard clicking) I'll pull the name and height inches columns, then we'll use the ntile function. The number of groups in parentheses that the ntile function returns can be customized, and I'm just going to stick with four groups. Then this is a window function, so we need an over clause and we have to sort the data within the window frame. (keyboard clicking) So, we're going to order the data within the frame by the height in inches. That finishes the calculation. I'll say from public.peopleheights, (keyboard clicking) and we'll order the results by height in inches. (keyboard clicking) Let me highlight lines seven through 10 and execute it, and we'll take a look at the results. Now, at first glance, the results look pretty good. We have the heights in ascending order. Everything at the lower end of the highest scale comes back as first quartile. Let's scroll through to find where they transition into second quartile. Row 101 is where that transition happens, where it switches from first quartile values up here to second quartiles down below. Earlier we calculated that second quartile starts at 63.7275, and that fits right between these two rows. So, that makes sense. Let's continue scrolling down to the third quartile rows. That transition happens down on row 201. Everything above is in second quartile, and everything below is in third. With the percentile function we calculated that the transition happens at 66.74. But, look at these height measurements. We have two people with the same height, 66.74. But one is shown in the third quartile, and the other one is shown in the second quartile. From a statistical standpoint, this doesn't make any sense because these two people are the same height. They should be in the same quartile. So, what the ntile function is actually doing is creating four groups with an equal number of rows. It's not taking tied values into consideration. The clue that this is what's happening should be that the row numbers where they transition is occurring at exactly row 101, 201 and 301. So, the ntile function isn't actually giving us statistical quartiles. It's just creating four even groups. The name of the function ntile in my opinion is pretty misleading. So, watch out for that if you perform these kinds of statistical calculations. To find the true quartiles that data points fall within, we'll need to use ranking functions, and we'll look at those later on in the course.

Find the most frequent value within a dataset with MODE

- The mode of a dataset is the value that occurs the most often, and it's another way to understand the central tendency of the data. PostgreSQL includes a mode function that finds this value, but its usage comes with a very large warning that you need to be aware of. Let's first find the mode of our height data, and then take a look at the problem with the calculation. The mode function itself doesn't take any arguments, but, like percentile, the mode function is another ordered-set aggregate function, so it uses the within group order by syntax. I want to find the mode of our height inches column and we'll pull that column from the public dot people heights table. Now, if we don't include a group by clause in this query, it'll simply find the most frequently occurring value within the entire table column. If I execute the query, the result tells me that 66.17 inches is the most common height. Now that may be good to know, but this value, all by itself, isn't all that useful. For instance, what the mode function can't tell us is how many times that height value occurs. Are 200 of our people the same height, or only two or three? The answer to that question can make a big difference in how we understand the data. But there's an even bigger problem with the mode function. To discover what that is, let's run another query. This time, I'll select the height inches column and count up all the rows. We'll do this by grouping all of the same values together that are found in the height inches column. I'll also sort the data descending. This will put the heights that occur the most at the top of the result. All right, let's highlight lines five through eight in this select statement and execute to see the results. So now we can see that 66.17 occurs three times in the data set, but we also have two other heights with multiple occurrences. 69.56 and 68.76 also occur three times in the dataset. The mode function only returns one value and it doesn't indicate that other values occur with an equal frequency. So that's the biggest problem with the mode function in PostgreSQL. If you read through the documentation, it clearly indicates that when multiple equally frequent values occur, the function will arbitrarily choose one for the result. This arbitrary nature of the results is a big red flag in my book, and the nature of this calculation needs to be fully understood before using this function in an analysis.

Determine the range of values within a dataset

- The range of a dataset is the last statistical measurement of central tendency that we have yet to look at. But truth be told, this is just a simple subtraction calculation, and there's no PostgreSQL function to calculate this value. I'm simply including this movie for completeness sake. To calculate the range of a dataset, all we need to do is subtract the minimum value from the maximum value. So to find the range of our height data, we could write out the query like this. It would say select, use the max function against the height inches column, and we'll subtract the min function, applied to the height inches column. That completes the calculation. Let's give it an alias, I'll call it height range, and we'll pull the data from the public dot people heights table. So that's all there is to the range calculation. I'll execute the query, and it tells me that my data has a range of 19 and a half inches. The max and min functions locate the highest and lowest values in the column, and then we simply subtract one from the other to get our range. Now, of course, we could take this a step further and break the range down by gender with a group by clause. I'll come back up to line number one and I'll enter in the new row gender. That'll pull that column out of the people heights table. Then we'll come to the end and we'll add in a group by clause. I'll group by roll up gender in parentheses. This time I'll execute the statement, and I get my range broken out by male, female, and the entire dataset. So this reveals that our height ranges for males spans 15.28 inches, and for females, there's a wider range from shortest to tallest at 16.49. And because we use the roll-up keyword, we're getting the full range of the entire dataset for both genders mixed together at 19.53. And with that, we've seen how to calculate all the different ways that you can find the average or a central tendency of a dataset by calculating the statistical mean, median, mode, and range.

Challenge: Retrieve statistics of a dataset with groups

- It's time for another challenge. Using what you've seen about calculating statistical information about a data set, I'd like you to analyze the Two Trees pricing data found in the inventory dot products table again. Your goal this time is to find the minimum, maximum, first, second, and third core tiles and the range of prices for each category group. This challenge is slightly different than the previous one, because this time you'll be using group aggregate and ordered set functions to arrive at the requested information. I estimate that this challenge should take about five minutes or less to write out the query needed for the challenge, and I'll share my solution in the next movie. Good luck!

Solution: Retrieve statistics of a dataset with groups
Selecting transcript lines in this section will navigate to timestamp in the video
(upbeat music) - [Instructor] I hope you are able to calculate the statistical information about the pricing data for the two trees database. Let's take a look at how I'd approached the challenge. I've opened up a new query editor here inside of pgAdmin. And as always, I'd just like to start with a comment to remind myself what it is that we're doing. So we're going, let's see, obtain some statistical information about product pricing. Okay, so we're going to start with the select statement and the challenge asked to have this information broken down by category ID. So I know that I want to reveal the category ID column from that table. So I'll type that in here. And just so that it's on the screen here, let's go ahead and open up the inventory schema. The products data table right there. We'll take a look at the columns. So here's all the columns we have to work with. We've got the SKU, product name, category ID, size, and price. But for this challenge, we're just going to be needing the category ID as well as the price column there. Okay, so we'll select the category ID. And let's see, we also know that we're going to be pulling this from the inventory.products table, so just fill that in right away. And we also know that we're going to be grouping our values based off of the category ID, so I can put the group by clause as well. And let's say rollup too. So group by rollup category ID, and so that's going to be the end of the query. So now all we need to do is fill in the rest of the columns that we want to see that calculates all the different stats that we requested in the challenge. So we asked for the minimum, maximum, the three percentiles, as well as the range. And I'm going to put this in sequence here, so let's do the min first. So we're going to find the min of the price. that just uses the min function and that's all we need for that one. So as, we'll call it min price, so that column. And at any point I can actually, if I get rid of this comma, I could actually just run this right now. Select it all, execute it. Make sure that I'm getting results, that I'm on the right track there. So we're getting results. I haven't made any mistakes so far so we can continue on. Next, let's calculate the quartiles. So we'll do the percentile. And I prefer to use the continuous percentile over the discrete percentile just in case we do have an even number of rows in the dataset. It'll average that together, but either one will get to a similar result. So we need to find our quartiles, so I'm going to use a 25% here to get the highest value in the first quartile. So, and then we need the within groups or within group syntax. Order by price being the type. Okay, order by price as, and we'll call this first quartile. So the reason we're doing an order by here is because within our group, we want to make sure that we're sorting the data within this kind of window frame that's being created for the percentile function. So it's going to take all of the pricing data. It's going to create its own little window frame for that data and then it's going to sort it. And it'll be sorting ascending so that it can find the value at the 25% mark along that dataset. So that's why all of that syntax is there within group and order by. Okay, so that finishes that column. We need to do the same thing for the second quartile. So percentile_cont, we'll do .5. Within group, order by. Price as second quartile. And one more time for the third quartile at 75%. That's third quartile, okay. So that's min, gives us our three quartiles. Let's do max now here afterwards. This is just a simple max calculation against the price. And the last thing that I asked for was the range and that's just the max minus the min. So max price minus min price as price range and that should do it. Let's get rid of all that extra space. Make sure I've got comma at the end of each of these. I do, that looks good. Let's go ahead and execute the query, and there it is. So here we have the breakdown for each category. So category one, two, and three. Minimum price across the category, the first quartile. So anything below 1,299 will be the first quartile for the first category. Below 1,199 will be in the first quartile for the second category, and anything below 899 will be in the first quartile for the third category. Then here's the breakdown for the second quartile. Here we go to the third quartile and then anything that's above this number but below the maximum price will be in the fourth quartile, and there is the price range for each category. And because I'm using the rollup keyword here, I'm also getting the overall row here on number four that gives me the overall minimum-maximum. The three quartile breaks, as well as the range for the entire dataset regardless of what category ID it is. So that's the answer that I came up with for the challenge. And once again, there's lots of different ways that you could get a correct answer with these kinds of queries. So you might have chosen a different approach which is completely fine. I'm going to save my solution as Challenge Three Complete in the exercise folder so that you can compare your results against mine.

RANKING DATA

Rank rows with a window function

- Putting values in sequential order and ranking data rows is a very common analysis operation. Ranking data is how you determine which products sell the best and which ones are underperforming. Or which items are the largest, tallest, heaviest, or most expensive and which items are the smallest, shortest, lightest, or least expensive. We rank things from top to bottom all the time. In PostgreSQL, the rank function can be used in a couple of different ways depending on the kinds of questions that you're looking to answer. It can be used as a window function to provide context in line with your data rows or it can be used as an ordered set grouping function to answer hypothetical questions. First, I want to look at using rank as a window function. Remember that window functions use the over clause and don't require a group by clause in the query. I'm going to pull the name and height_inches column from the people_heights table. I'll also sort the data descending. The results will give me a list of all of our people with the tallest person at the top and the shortest person at the bottom. We can formalize this ranking by adding an additional column to the results. Now there's two different functions that do this and they give slightly different results. Let me come back up here into the query and we'll add in the first function. The rank function creates the window frame and because it's a window function that we're using, I need the over clause. Then we need to sort the data within the frame by height_inches. I also want it to be in a descending order. This way our rankings will stay sequential from tallest to shortest no matter how we sort the final results set. The other function that we have available is called dense_rank. It uses the exact same syntax, so I'm going to go ahead and add that in and we can compare the results. So there's our two functions. We have rank over, order by height_inches descending, and dense_rank over ordered by height_inches descending. Let me go ahead and execute the query and we get two new additional columns added into the results. These give us a numerical representation about how each person falls in the height rankings. We can see that Marcellus is the tallest person in our dataset using both of these functions. And because we're sorting the height values within the window frame, we can flip the results around to sort by height ascending instead by changing it to ASC here in the order by clause. If I rerun the query, the values get sorted in the opposite order but you can see that their ranks are still intact. If I scroll all the way down to the very bottom of the list, we'll see Marcellus down here at the bottom again is still ranked the tallest. We can even sort the data alphabetically by person name and their height rankings will still get reported correctly. I'll change the order by clause to order by name, rerun the query. Now you can see that everything is alphabetically based on the person's name but the ranks are still intact. I'll switch this back to sort by the height_inches descending. Okay, so now let's take a look at what the difference is between the rank function and the dense_rank function. Both columns look like they're returning the same value, and for the most part that's correct. The difference is in how the two functions handle multiple rows with the exact same value. In these results, scroll down until you get to rank number 30. Here we can take a look at Galen and Clayton. Galen and Clayton both have the exact same height, 72.13 inches. Both functions rank them the same and they're shown as they're tied for the 30th tallest person. But look at the next row for Rosario. Rosario is ranked 32 by the rank function and 31 by the dense-rank function. Dense_rank doesn't skip ranks when there are multiples with the same rank. The rank function does. Whether you use one function or the other depends on how you want to interpret the results. The rank function says that Rosario is the 32nd tallest person which is correct in the sense that they're clearly shorter than both Galen and Clayton. However the dense_rank function says that Rosario has the 31st ranked height which is also correct if you're only considering unique height measurements. So the results for each function can both be correct in their own way. Now because we're using rank as a window function, we can also partition the data within the window frame. If you wanted to rank everyone within the same biological gender, you can add that into the over clause. Right before the order by keywords, we'll add a partition by statement and we'll partition by the gender. I can do the same thing for the dense_rank function. We should also then include the gender column in the output results by adding that column into the select clause. I'll add it right after height_inches. And I'll also sort the final results by the gender values so that the results display in a sequence that's easier to understand. I'll add it here to the beginning, so we'll order by gender first and then the height_inches descending. This gives us our final height rankings within each person's gender classification. At the top, we have the tallest females and if I scroll through about halfway through the dataset, we'll find the tallest males. They start right here on line number 201 with Marcellus at the top of the male list again. So using rank as a window function allows us to display row rankings in line with the other columns from the database.

Find a hypothetical rank

- Using rank and dense rank as a window function with an OVER clause will add numeric rankings in line with your other data columns. You can also use rank and dense rank as an ordered set function within a group. This will allow you to answer hypothetical questions. Now you may be wondering, "What exactly is a hypothetical question?" With this technique, we could get rankings for values that don't actually exist within the data and find out, hypothetically speaking, where they would rank if they were in the dataset. I think an example will help clarify what I mean. Let's take a look at the height data again. I'll select the name and height inches columns from the people heights table and we'll sort the data descending. So here's the data that's currently stored in the database. Marcellus is the tallest person at 76.87 inches tall. The second tallest person is Steph at 74.74 inches. Now, let's suppose that we measure the height of a new person and they are 75 inches tall. You might want to know where they would fall in the ranking if we were to add them into this dataset. At 75 inches, you can see that they would be the new second tallest person, right behind Marcellus. Steph would become the third tallest at that point and everyone below would move one position down in the ranking. A hypothetical ranking will tell us where the new row would go if we were to add it to the existing dataset. So let's see how to do that. I'll start up a new select query. Now to use rank as a hypothetical ordered set aggregate function, we need to put the value that we want to insert within the parentheses after rank. So let's evaluate the height of 75 inches. Then, because we're using this as an aggregate set function, we use the within group keywords, just like we did with the percentile function. Next, we need to sort the data within the group by the column our hypothetical value would go in. In this example, that would be the height inches column and we want a ranking from tallest to shortest, so we need to make sure that we sort everything descending. So that completes the calculation, all that's left is to specify the table where our data is coming from. So that'll be the public schema and the people heights table. So let's run this select statement. The results tell us exactly what rank 75 inches would be if we added it into the dataset. And just as we imagined, it would assume the second position in our rankings. The within group keywords in the function tell the function to operate on rows that fall into each of the queries groups. But since we didn't include a group by clause in this query, it's evaluating the height of 75 inches against the entire dataset. If we were to add in a group by clause into the query, we can find out how that height would rank within different classifications. For instance, we know that 75 would be second overall, but how would it rank for males compared to females? We can do that by adding in the gender column into the select statement, then I'll add in a group by clause to the end. Let's group by rollup and gender. Now I can run the query again to see the difference. The results tell me that someone with the height of 75 would be the second tallest male, but the first tallest female. We can now easily test other heights to see, hypothetically, where they would fall. Let's test the height of 70 inches. I'll change that in the rank function up here on line seven, then we'll run that query again. Someone with the height of 70 inches would be the fifth tallest female, the 83rd tallest male, and the 87th tallest person overall. So by using the rank function as an ordered set aggregate, we can answer hypothetical questions to see how values would rank if they were added to the table.

View top performers with percentile ranks

- In the previous chapter, we looked at calculating the values that define the breaks between different percentiles and quartiles. We also looked at end tile window function that divided our data into equal groups. But we saw that it really wasn't suitable for determining usable statistical quartiles in line with the data. In order to break our dataset into usable quartiles, we first need to use a percentile ranking function. This will reveal the percentage of rows that are above the current row in a stack rank. I'll start by selecting the name, gender, and height columns from the people_heights table. Then we'll add another column to the output. For this, I'll use the percent_rank function. The over clause will create a window frame for my data. Now, I don't need to partition it, but I do need to sort it, based off of the height_inches descending. This will calculate the percentage rank for each row in my table when compared to all other rows in the table. To finish up the query, I also want to sort my final results descending, so that the tallest people appear at the top. The results of this query will show all of my heights and the percent rank of each row. This tells us how many rows as a percentage of the entire dataset are above the current row. So, for Marcellus, the tallest person in the list, 0% of the rows are above theirs. For Steph, only .25% or one-quarter of a percent of the rows are above theirs. So now that we have each person's percent rank, we can use these values to divide everyone into quartiles. We can do this with a case statement that'll evaluate the results of the percent rank function and return a bit of text, depending on where each person falls in the ranking. A case statement is a good way to build some logic into the query and alter the output based off of the result of a calculation. They're basically like an if-then-else block of code that you might use in a programming language. So in this example, when a person's percent rank is less than 25%, then they'll be in the first quartile. If they're less than 50%, they'll be in the second quartile, and so on. The case statement will automate this evaluation and help us place each row into the appropriate bucket. To add one into the query, I'll come back up here to line number 2, type in a comma here, and come down to 3. This is where we'll start the case statement. Each line in the statement will start with when, so when the result of this function is less than .25, then they'll be in the first quartile. I'm just going to highlight everything here on line number 2 except for the comma, and copy that to my clipboard, and paste it after when. So when this function is less than .25, then we'll say they're in first quartile. Now we can evaluate the second quartile. And for this, I'll copy everything that's on line number 4 and I'll paste it onto 5. This time, we want to find everybody that's less than 50%. And we'll say that they're in second quartile. Next, we can evaluate the third quartile. And the break for that is at 75%, so .75. And they will be the third quartile. Finally, we can say else, so everybody else that isn't in the first, second, or third quartile, we'll say they're in the fourth quartile. That finishes the case statement, so I'll come down to line number 8 and say end. And I'll give the column an alias. I'll call it quartile rank. The important thing to note here is that all of the text that we're returning, first, second, third, and fourth, get wrapped inside of single quotation marks, but the alias, the name of the column, gets wrapped inside of double quotation marks. The other thing to note here is that each of these lines do not end with a comma like you might be used to. Okay, let's execute the query and take a look at the result. So here is our new quartile ranking. Now we have an easy way to see which quartile each of our people fall into when ranked by height. If you remember back to the discussion on the end tile function, the problem that we encountered was that it didn't properly account for ties, or multiple occurrences of the same value. Let's see how this new approach handles the same situation. Scroll down in the results to where the second quartile ends and the third quartile begins. Here, Shelba and Malia have the same height, and fall right on the transition between the quartile groups. In the end tile function example, they were arbitrarily split up into different quartiles. With this improved approach with the percent rank function, they're kept together in the quartile ranking exactly as they should be.

Evaluate probability with cumulative distribution

- There's one additional ranking function that we should take a look at before wrapping up this chapter. It's called cumulative distribution, and it's almost identical to the percent rank function, except for one small detail. Let's take a look at the result of both functions side by side. I'll start with the same name, gender, and height inches column from the People Heights table. I'll also add in the same percent rank function that we used in the prior movie. Next, I'll add in the cumulative distribution function, which is abbreviated as cume_dist. This function works exactly the same as percent rank. It uses the same over clause and the same order by statement. Finally, I'll add an order by clause to the very end of the query to sort the height values descending. Okay, let's execute the query, and we'll take a close look at the results. You can see that these two columns return numbers that are very close to each other. They both start out very small here at the top, and if I scroll all the way down to the very bottom of the list, they both end at one. Both of these functions are calculating a form of percent ranking. The difference is that percent rank excludes the current row from the calculation, whereas cumulative distribution includes it. Let's go back up to the top and we'll check the actual math that's being performed. I want to focus on the results for row two. The cumulative distribution is found by taking the row's position and dividing by the number of rows. There are 400 rows in this data table, so two divided by 400 is 0.005. That's the number that we get for this calculation for cumulative distribution. Percentile rank divides the number of rows above the current row by the number of rows in the table, excluding the current row. This makes the calculation one divided by 399. If you break out a calculator, you'll see that one divided by 399 is 0.002506265, and so on. So that's why these two functions returned values that are slightly different, with the exception of the final row, which will always be one. The main takeaway here is that cumulative distribution returns the percentage of rows that are less than or equal to the current row, while percent rank returns the percentage of rows that are less than the current row, while also removing the row from the calculation. It's a subtle difference that may or may not make a difference in your statistical analyses, but it's good to know that it exists.

Challenge: Evaluate rankings within a dataset

- Now that we've seen a number of ranking functions, let's apply that knowledge to another challenge. Return to the inventory.products table and rank all of the products by price. I'd like you to write a single query that shows product rankings overall, segmented into their category groups, and segmented into size groups. I estimate that this challenge should take about five minutes or less to complete, and I'll show you my solution in the next movie. Good luck!

Solution: Evaluate rankings within a dataset

- I hope you were able to write out a query that ranks the product price information on a number of different segments. Let's go ahead and walk through the solution. I'm going to start by taking a look inside of the inventory schema, where we'll find the products table. So here are the different columns from that table that we can work with. And as always, I just like to write out a quick comment here at the top, so I can remind myself what it is that we're trying to do. So I need to rank product pricing overall and by the category, as well as by size. So we're ranking the product pricing overall by category and by size, so that's what we're trying to do here. Okay, let's start pulling out a couple of columns from the product table. And I'll take a look at these columns. I don't think I need the SKU, but I do want to see the actual name of the product, so we'll just include that one. So product name. Category ID, we are going to be ranking based off of category ID, so it'd be nice to see what the original category ID for each product is, so I'll include that in our result. Same thing with the size, we're going to be ranking on size, so it will be nice to see the original size. And I would also want to see the price. So those are the four columns that I'll pull from that table. And now we can get started with our ranking functions. Now, we have two options here. We could either use the rank function or the dense rank function. And I don't think it really matters which one you choose for this as long as you know how to interpret the difference in the result and know that dense rank won't skip numbers for values that are the same, but rank will. So I'm going to stick with dense rank for my solution, but if you chose to go with rank, that's fine as well. So I'm going to go with dense rank here. So the dense rank function, it's a window function, so I'll use the over clause. And for the first one, we want to get the ranking of the product pricing overall, so I don't need to segment or partition this particular window. So I'm going to exclude a partition by clause. We'll just skip right to the order by clause. And I want to order by price and descending. That way, the highest price gets the highest rank. Let's give this an alias as well. Lets call this as rank overall. Okay, that's the first one. So dense rank again, over. Now this time, we're going to be ranking by the category, so I do need a partition for this one. So, partition by and category, the column is called category ID. So partition by category ID. And then we also want to order it. Order by price descending. And let's call this rank category. Okay, and then one more. Dense rank over partition by, and this time we're going to be partitioning by the size, and size is the name of the column. Partition by size, order by price descending. Give it an alias as rank price. All right. So that finishes all of the columns. So we've got four columns coming out of our raw data, then we've got three columns that are coming from these window functions, and we'll just pull it all from the inventory.products table, and let's sort the results. Let's sort it by category ID. And actually, within the categories, let's sort it by price as well. Let's sort it by price descending, that way, the highest price within each category will appear at the top, but then when we switch from category one to category two, the pricing will re-sort again within each category. And that looks good. Let's go ahead and execute, see what happens, all right. So here are the results. So we've got category one here at the top. We sorted by price, so it's the largest price, which just happens to correspond with the largest sizes. And we can go through and start to interpret these results. So it looks like $27.99 is the highest price overall for the entire dataset. It also is the highest price for the category one items, and it's also the highest price for our 128-ounce products. So those are all the same. Looks like these are all the same as well across all three segments. Let's scroll through and see if we can find something that's more interesting. Okay, here we go. So here's the price of $23.99, is the fourth highest price across the entire data set. It's the fourth highest price within category one, but it is the highest price for the 64-ounce products. Let's go ahead and scroll down some more. Let's go to a different category. Let's find category number two somewhere in here. There we go, there's category two. Okay, so we've got the price of $19.99, is the eighth highest price overall. It is the first highest price in category two. So there are no prices in category two higher than $19.99. And it is also the first highest price for the 32-ounce size. So again, for any products in the 32-ounce size, the highest they go is $19.99. And that is my solution. So, that's my solution using the dense rank function. You might've gotten something different if you ran with the rank function. And I'm going to save this as challengefourcomplete.txt, and you can compare my results against your results.

DEFINE OUTPUT VALUES WITH CONDITIONAL EXPRESSIONS

Define values with CASE statements

- In the previous chapter, we used a Case statement to help us group ranked data into Core Tiles. Let's take a closer look at the construct of a Case statement and see other ways where it could be useful in PostgreSQL. Now Case statements allow you to add an if, then, else logic into your queries. This allows the query to evaluate a condition and return a different value, depending on the results of that evaluation. A basic Case statement would look like this. First, it'll create a column in your query results, so treat it just like any other table column or function that you might add into the select clause of a query. Then, inside of the Case statement, you'll add when and then clauses. The when will be a condition that you'll want to evaluate and it either needs to be a true or false statement. The then portion will control what to respond with if the when condition is true. If the first when condition is not true, then the Case statement will move on to the next when condition. If that one is true, then it'll return whatever is written in the following then portion of the statement. You can continue to add as many when and then clauses as you want. At the end, you can include an optional else keyword to capture all other conditions. After that, the case statement is complete so you need the end keyword, and that's the entire construct of a Case statement. So let's fill in some details and see how it actually works. I'm going to come up here, to the first when statement, and I'll type it in the formula zero is equal to zero. Now this will, obviously, return to true, but we'll go ahead and fill in the rest of the details just to make sure. If zero is equal to zero then we'll output a text, 'A', and I'll just wrap that in single quotation marks. The next when condition I'll type in as one is equal to zero. For the second then statement I'll type in a 'B', inside of single quotation marks, and for the else I'll type 'C'. When you execute the statement should be no surprise that the results come back with A. That's because the first when condition is true, zero is equal to zero. Let's mix it up a little bit. I'm going to change this to one is equal to zero and when one is equal to one. This time when I execute the statement, it returns B. The first condition is no longer true, so the second when condition is then evaluated. This one is true, one is equal to one, so the Case statement returns the letter B. Now try two is equal to zero, and two is equal to one. This time when I execute it, it returns C. Neither of our two when clauses are true, so the else portion of the statement steps in to supply the end result. So what happens if multiple when clauses are true? Let's test that out. I'll make the first one say when zero is equal to zero, and the second one is when one is equal to one. This time when I execute it, it returns A. The sequence of the conditions evaluated in each when clause is important. The Case statement will return the value for the first when condition that is true. In this case, it's not even getting to the point where it's evaluating the second when right here. So now that we have a good idea about how the Case statement works, let's put it to work with a query for the two trees data. I'm going to highlight all of this and get rid of it. First, let's take a quick look at the inventory.categories table. The results, of a select star, from inventory.categories query shows me the different categories for our products. We can see that our products fall into three categories, one, two and three, which corresponds to Olive Oils, Flavor Infused Oils and Bath and Beauty products. Now if we query the inventory.productstable, you'll see that we have a relationship based off of the category_id column. This shows me the category value, but the actual name of the category isn't stored here. Now this is good from a relational database design perspective, but these category_id numbers are not very user-friendly. You'd have to remember what category one, two and three actually means. Now we could write a query that joins the two tables together and pulls the related category description value for each product, or we can use a Case statement to get the same result without having to perform a table join. It will be written out something like this. Let me get rid of this portion of the select statement, and we'll start pulling a couple of columns. I'll start with the sku, product_name and category_id columns, then we'll follow that up with a Case statement. For the first when condition we'll say when category_id is equal to one then it'll output the text, Olive Oils. When the category_id is equal to two then output Flavor Infused Oils. And when the category_id is equal to three, then Bath and Beauty. Now, at this point, we only have three categories so it should never get to an else, but I'll put that in just for good measure. We'll say else, category unknown. That finishes the Case statement so we'll put it in the end keyword, and we'll give this new column an alias, I'll call it category description. So that adds an entire column based off of this Case statement. At the end of this, I can type in a comma and add in additional columns from the original table. For instance, I'll add in the size and price columns. Next we need to specify the table where all of this is coming from, so we'll say from inventory.products. The results will display the correct category description for each category_id. The Case statement is evaluating the category_id for each row as it processes the results. The current rows category_id value gets plugged up into the where conditions, and the first one that matches gets returned. Every row is processed individually, so, as you scroll through the results, you'll see Olive Oils up here, where the category_id is one, then, as we scroll down, we should find category two is Flavor Infused Oils, and, at the bottom, category three is Bath and Beauty. So, using a Case statement, we have the database engine help us out in evaluating different potential situations, and it'll alter the output based off of those results.

Merge columns with COALESCE

- The coalesce function can be used to choose an output from multiple columns, if those columns may contain null values. It returns the first non-null value that it finds. The function looks something like this. We'll select the coalesce function. And I'll pass in three pieces of text: the letter A, B, and C. Inside of the parentheses, you can list out as many or as few options as you want. Here, I'm just hard-coding the letters A, B, and C, but these are more typically column references to a table. Then, when the function is executed, it outputs the first value that isn't null, or empty. In this case, that's the letter A. I'll change the query to make the first option null and then run it again. This time, the first value is null, so it skips to the second value, and that's exactly what it returns, B. If the second value is also null, then this query is going to return C, and if all of the options within the parentheses are null, then the coalesce function will return null. So that's how the coalesce function works. Let's demonstrate how this can be useful in a query. First, I'm going to review everything that's in the inventory.categories table. This shows me that our two trees database has three categories currently. I'm going to add in a new category for a gift basket product line, but I'm not going to give it a category description. To do that, we need an insert into command. And we're going to insert the value into the inventory.categories table. The values that we're going to put in will be the number 4, a null value, and the text Gift Baskets. I'll execute this insert statement. That'll add the new row into the table. And if I select everything from inventory.categories again, we'll see that new row right there. So now we have a null value for the category description for category number 4. This is exactly how the data is stored in the table. However, using coalesce, we can write a query that will substitute the product line text for the description anywhere that encounters a null value. To do that, we'll write out the query like this. I'll start with the category ID column. Then I'll create a new column, using the coalesce function. We'll prioritize the category description column. But if that one is null, we'll use the product line column. We'll give this new column that we're creating with the coalesce function a name, and I'll just call it description. Then we can continue on, adding additional columns to the query. For instance, I'll pull the original product line column. We'll finish the select statement with the from clause from the inventory.categories table. And I'll highlight these four lines and execute it to see the final result. So instead of pulling values from only the category description column, I'm using the coalesce function to prioritize the category description. But if that's null, it will return the value from the product line column as a fallback. And that's exactly what we're seeing right here in the results. The null value is being replaced with the text from the product line. Notice that the other three descriptions weren't affected. The coalesce function allows us to effectively patch the hole in the data by substituting another value for the null.

Convert values to null with NULLIF

- Where the coalesce function turns null values into something else. The nullif function does the exact opposite, it turns non-null values into null, here's what it looks like. We'll select the nullif function and it only takes two values or two parameters, I'll type them in as A and B. The first parameter is evaluated against the second. If they're different than the first value is just passed straight through. If I execute the query right now, a is not the same as b. So the nullif function returns a. However, if both values are the same I'll change it to nullif a,a and re-execute when they're both the same, nullif returns null. To see this apply to some data, let's take a look at the inventory dot products table. I'll just grab everything out of the table and execute it. So here we can see all the data exactly as it saved in the database, all of our products are available in a wide range of sizes. We have eight ounce products, 32, 64, and so on. And you can see that these sizes are repeated over and over again for different products. Now using nullif, we can eliminate a specific size from the results, it would look something like this. Let's select these SKU, product name, and category ID columns. Then we'll use the nullif function. I'll use it to process the original size data, and we'll look for the size 32. When it encounters it, it's going to replace it with a null value. Otherwise it'll just output the original sizes. This is going to create a new column in the output results. So let's give it an alias, and I'll just call it size. Then we'll add in the price column from the original table and everything is coming from inventory dot products. Now I'll execute this query. We'll take a look at the results. Everywhere that there was a size of 32 ounces, the value has been replaced in the query results with a null value. Okay, I'll admit that this is a bit of a contrived example, but here and there, nullif does have some good uses. In scientific data collection, it's not uncommon to have a dataset with something called a canary value. For instance, an automated thermometer probe might collect temperature data every hour. If the sensor malfunctions, the reported temperature may get stored as something like negative 9, 9, 9 degrees. This value is so far outside of the expected range that it's instantly recognizable by an analyst as a malfunction and not a true measurement. These types of data anomalies are sometimes called canary values because like the proverbial canary in a coal mine, they can provide early warning signs of a potential problem. With the nullif function, you can essentially scrub these values from the dataset while performing an analysis, and convert them into null values that won't impact a statistical calculation.

ADDITIONAL QUERING TECHNIQUES

Output row numbers with query results

- When you execute a query, depending on the graphical interface you're working with, you'll typically have a list of numbers down the left side of the results that indicate each rows number. For instance, if I select all of the product data, when I execute the query here in PGAdmin, the left of the data output grid shows each row's position in the results. These numbers aren't part of the actual data. They're just shown as a convenience here in the graphical interface. However, PostgreSQL does have a row number as function that'll add this kind of sequence number as a column in the result set, which makes it convenient if you want to export the data to a spreadsheet or incorporate it into a report. To add a column of row numbers, first, select your table columns as usual, then we'll create a column using the row number function. This is a window function, so it needs an over clause. That creates an independent window frame of the data that we can sort and group within. If we say order by sku, it'll create a sequence over the entire data set. Then we just need to finish the query with our from clause. In the results we'll have a new row number column. This column fills in the sequence and makes it part of the actual data and not just a feature of the interface. And because this uses a window function, that means that we can partition the window frame to create sequences that restart for every group. For instance, we have lots of different products with the same name. If we come back up into the over clause and add partitioned by product name, when I re-execute the query, it'll give you a slightly different result. Now, the row numbers only count up within this same name classification. We have our Basil-infused oils at one, two and three, then it switches to the bold oils at one, two, three, four, and if I scroll down, we'll see the fifth one there. Every time we get to a new name, the sequence numbers start over again at one. So having row numbers attached to your queries output can be a convenience whenever you need to call attention to specific rows in a report or other analysis, especially when you can't rely on those numerical row identifiers being a built in part of the interface.

Cast values to a different data type

- When you execute a query, the data types of the output columns will be the same as the data types established in the original table. In some graphical interfaces, this is made explicit in the column headers. You can see this here in PG Admin, if I select everything from the orders table. In the output results, in the column headers, PG admin displays the type of data that each column contains. The order ID is an integer. Order date is a date data type. And customer ID is a character data type. Now it's possible to convert these data types on the fly, which can be useful if you need to combine values from multiple columns using a coalesce function, or a case statement. Or, if you simply need to make different kinds of data compatible. You can convert the mismatched data types to a common type to work with them together. So right now the order date column is storing dates. This allows PostgreSQL to calculate elapsed time, and help us restrict input to only valid dates. For instance, PostgreSQL would not allow us to input a date of March 38th. But sometimes we need to work with these values as a basic string of characters. Just a sequence of digits and hyphens that will no longer have any deeper meaning to PostgreSQL. We can do that by converting this output into text. In the query come back up to the order date on line number two. After the reference to the table column, type in two colons. This is PostgreSQL type cast operator. It's used to convert data from one type into another. After the colon, fill in the name of the data type that you want to convert to. In our case, we just want to convert the data into text. And that's all there is to it. I'll re-execute the query, and we'll take a look at the result. Here, you'll notice a slight change in the output. The column is now indicating that it's returning text instead of a date. So even though the actual values haven't changed, as far as we can see, PostgreSQL is now treating the date data as text instead of dates.

Move rows within a result with LEAD and LAG

- There are two functions that will take a column of values from a table and then shift them up or down in the results. These functions are useful when you want to compare or reference values stored in adjacent rows. We can take a look at an example of how this works by pulling order records from the sales.orders table. These results will be sorted chronologically by customer, so that we can see the sequence of orders placed by each customer over time. Let's take a look at the first customer, BV446. They placed an order on March 19th, a second order on the same day, and a third order on the 26th. Now for each of these rows it would be convenient to be able to see when they placed their previous order and when they placed their next order. This would help us calculate how much time has elapsed between purchases. In order to calculate the time span between orders for each customer, we need to have the subsequent order dates shown on the same rows. If we come up into our query, we can add in a second copy of the order_date column, but when I execute this query, we'll just get a duplicate of these same values that we already had. What we need to do is shift the values up or down in this second column. We do this with the lead and lag functions. Lag will shift the values down within a column. This will allow us to compare dates for each customer's next order. To use it, I'll come back up here into line number four, and we want to lag the order_date values. The lag function is customizable. We can specify how many rows we want to move the data up or down. I only want to lag the data down one row, so I'll type in comma, 1. Now this is a window function, so we do need an over clause. within the window frame we need to create a partition. Otherwise PostgreSQL will just shift all of the dates down by one, without regard for the specific customer. We only want to lag the results within each customer's order history, so partitioned by customer_id. We also need to make sure that they're in sequence, so we can order the window frame by the order_id. Now for this dataset, you could also sort based off of the order date, since both values place the data in rows of the same sequence. It really doesn't matter which one you use here for the order by clause in your window function. Let's finish this column off with a good name. I'll call it previous order date. Now we can use the other function, lead, in order to find the next order date. The lead function works exactly the same way, but it shifts values up in the column. We'll lead the order_date values up by one, then we need our over clause. We'll partition by the customer_id again, and order by the order_id. I'll give this column an alias as next order. In order for everything to fit on the screen, I'm going to actually insert an extra line break here before the as alias, that way you can read everything all at once. Now let's execute the query and take a look at the results. The first row shows customer BV446's first order. It was order number 102, and it was placed on March 19th. Since this is their first order, there is no prior order and we get a null value in this column. The next order was placed on the same day, 3/19. Here's the row for that order. It's order number 103. We can see the date of the prior order, as well as the date of the next order, the 26th. You can continue reading through the rows to find out exactly when each order was placed and when the proceeding and following order was placed, if there is one. So now that we have these dates, we can calculate elapsed time between them. I'll come back up into the results and we'll add in one more column. In order to calculate elapsed time, we simply need to subtract one value from the other. I'll take the calculation for the lead function, I'm just going to copy it onto my clipboard, then I'll paste it onto line number eight, and from that, we're going to subtract the current order date. Let me put that on the next line. Then we can give this column an alias. I'll call it time between orders. So in this column, we're using the lead function to calculate the next order date, then we're subtracting the current order date, and we're creating that as a new column. Let's execute and take a look at the results. So now we can easily see how many days have elapsed between orders. between the first and second order, there were zero days. Between the second and third order, we had seven days. If we scroll to the end of the results for BV446, we can take a look at what that looks like. Here on line number 12, we have their final order. It was placed on April 28th. The prior order was on the 26th, but there is no next order, so there is no time to the next order. That finishes all of this customer's details. And we can take a look at the next customer, BX305. Their first order was on the 15th. There is no previous order. The next order is on the 25th, with a time span between of 10 days. So as you can see, since the lead and lag functions use windowing and partitions, these calculations are performed only within the customer's order history, without getting the dates mixed up between customers as they get shifted up and down in the results.

Use an IN function with a subquery

- When filtering the results of a query using a where clause, you might find yourself listing out multiple criteria for the same column. For instance, let's suppose that I wanted to take a look at some product data. If I only wanted to see products with three specific names, I could write out the where clause like this, where our product_name is equal to Delicate, or product_name is equal to Bold, or product_name equals Light. The results of this query will show me exactly those items. Here are all of my delicate items, my bold items below. And if I scroll to the end, we'll see all of the Light items. A more streamlined approach that'll get the exact same result is to use a function called in. The in function takes a list of items and it'll compare the column against every item in the list. So all of these or lines will get rewritten like this with an in function. We can say where product_name in, open a parentheses, and then list out the three items, Delicate, Bold, and Light. We'll finish the function with a closing parentheses and I can get rid of these other two or lines now. The end result of this function will be exactly the same. Here, we have Delicate, Bold, and Light at the bottom. If the product name is in this set of options, then it gets returned. If the product name is not in this list, then it's filtered out and excluded from the results. The in function can also take a select statement that returns a single column. If you wanted to see product details only for products that have five or more items within the same group, you can first find out which product names those are. We'll start with another select statement. This will use a counting function and a group by product line. When I execute this query, we can see the results. Here, we have a list of all of our product names with a count of how many items have the same name. We can see that many products have five items within them. Here's one that only has three. And you can scroll through the list to see how many products are in each name. Now, if you wanted to filter this list to only the product names that have five or more items, remember that we need to use a having clause to filter out the groups. I'll add that to line number eight. So we'll find all the groups where the count is more than five. This shows me this limited list of items. And you can scroll through the list to see that they all have a counting of five. Now we can remove this count function from the list of columns and that'll return just a single column of values. All of these names have five or more items in their group. Since this query is now returning a single column, we can use this entire select statement in the in function up above. Let me copy the entire thing to my clipboard, then I'll come back up into the in clause and inside of the parentheses, I'll paste it in. Let me space things out so it's a little bit easier to read what's happening. I'll make sure that I remove the semi-colon from this inner select statement and will move the closing parentheses down for the in function. So this inner select statement on lines four through seven will fetch the list of items that we're interested in. Then the where clause will filter the outer queries results to just the items in that list. Let's highlight everything from one to eight and we'll execute to see the final result. Now, no products are shown if they're in a group with less than five items. With the in function and either a list of items or a sub-select query that returns a single column, you can streamline your where conditions.

Define WHERE criteria with a series

- PostgreSQL can create a list of sequential numbers with the generate_series function. You can customize the starting and ending number as well as the interval that numbers will be created. For example, we'll select generate_series and inside we have a couple of options. The first number is the number the series will start with. Let's start with 100. The second value is the ending number, and I'll make a series that goes up to 120. The results of this query will list out all the numbers between 100 at the top and 120 at the very bottom. You can also add in an optional third parameter to control an interval. After 120, I'll type in comma 5. This will create a series that will range from 100 to 120 but only includes every fifth number. Now right now, I'm treating the generate_series function as a column with no table. But you can also treat it as a table with a single column. You might see the same function written out like this. Select * from generate_series 100, 120, and 5. The end result will be exactly the same. I don't think it really matters at all from a performance perspective whether you think of the function as a column or a table. Now this list of items can be helpful if you only need to perform a statistical analysis or an audit of a subset of your data. Say you wanted to perform a spot review of every tenth order. You can use the generate series function with a where clause by combining it in an in function. The query might look something like this. We'll select * from sales.orders where the order_id is in and open a parenthesis. Then inside of this function, we can use the generate_series function. I'll create a series that ranges from 0 up to some arbitrarily large number. How about 10,000? But I only want every 10th value, so I'll type in another comma and 10. This creates a list of numbers. The in function then compares these numbers against our order_ids. If I execute the query, we'll see every order in that series. That returns order 120, 160, 140. And actually, these are in kind of a weird order, so let's add an order by clause into the results. I'll execute again and that gives us a better sequence there. So now you can see we're only pulling every 10th order. Instead of starting the series at 0, we can change this to 3 and re-execute. Now the results will still skip 10 rows between, it just starts at a different number. Now we're getting 103, 113, 123, and so on. Finally, we can use this technique with dates as well. We can find all orders where the order date is in a series. The generate series function can create a list of dates within the range. Let's change this where clause to take a look at the order_date column instead of order_id. Then we'll select generate_series and change these parameters in the parentheses. We'll start with March 15, 2021. Now the only trick here is that we need to convert this into a timestamp data type. You'll recall that PostgreSQL uses the double colon notation to cast data into a different type. So we'll change this text value, 2021-03-15, into a timestamp. So that's our first parameter in the range. The second date in this range will be March 31st. I'll convert this to a time stamp as well. And for our interval, let's say I want every five days. That gets written inside of single quotation marks as well. Since we're working with time stamp data, I can say 5 days, or you could say something like one week or a number of months. Now if you want to see exactly what values this generate_series function will return, just highlight everything on line number four and we can execute it by itself. This shows me all of the days that that select generate_series function will return. Now we can find all of the orders that were placed on those specific days by running the entire query. And here we go, every order that was placed between the 15th and the 31st of March, skipping every five days. The generate series function is useful anytime you want to generate a list of numbers or dates that can be used elsewhere in your queries.

Challenge: Calculations across rows

- We are almost at the end of this course, so let's wrap things up with one final challenge. I'd like you to look at the height data in the people heights table. Sort everyone from tallest to shortest, then figure out how much taller each person is than the person below them in the list. This challenge should take between 5 and 10 minutes to complete, and I'll share my solution in the next movie. Good luck!

Solution: Calculations across rows

- I hope you were able to perform the calculation that compared each person's height measurement. Let's take a look at how to solve this challenge. I'm going to select a couple of columns from the people_heights table. Actually, let me open this up just so we can review what columns we have to work with. So I'm going to select the person_id, and the name, and the height columns. I'm not going to be worrying about gender for this particular example. So we can leave that out of the list. So I'll select those three columns from public.people_heights, and I wanted to add a sort into this. So we'll order by the height in inches column. And I want to do this descending so that the tallest people appear at the top. Okay, let's go ahead and sort this out, and there is our friend Marcellus again who's at the very top of the list, followed by Steph. So now part of the challenge is to figure out how much taller each person is than the person below them. So I want to compare Marcellus against Steph, Steph against Joseph, Joseph against Jorge and so on. So in order to review that data in the same row, we need to use either a lag or a lead function in order to get a copy of this data, and move it up or down in the sorting so that it appears on the same row as each other person. So let's go back up here into column number three and add that in. Now, it might be a little bit confusing whether you're supposed to use a lead or a lag function for this. Lag will show values from rows below the current row, and lead will show values from above. And so for Marcellus's row, we want to see Steph, which is from the row below. So that tells me that I need to use a lag function for this. So we can use a lag function. And let's pull the person's name first and then we'll take a look at the height next. So let's lag the name column by one row, and it's a window function so I need the over clause. And I need to make sure that the column is sorted in the same way so that we're grabbing Steph for Marcellus' row and Joseph for Steph's row. So we're going to add an order by clause into the over clause. So order by, and it's going to be the same sorting as we're using for the main query. So it's going to be sorted by the height_inches. If we were to sort based off of like alphabetically by name, then the sort within the window created by the over function will be in a different sequence, and you'd wind up with some other random name, just whatever happens to be alphabetically near each person's name, not in the same listing as the heights. So that's why we're ordering by height_inches. Okay, so we're going to lag the name by one row over the sorted list by the height. And let's just go ahead and run this right now. And we'll make sure we're getting good results here. So let's execute the query. Okay, so this is the lag column. We haven't named it yet. But I am, in fact, seeing Steph on Marcellus' row, Joseph for Steph's row, so we're on the right track here. Let's give this a name. So we'll call this as is taller than. That way we get a good result here. When I execute the query, it says okay, here's the row for Marcellus. There is Marcellus' height, and then is taller than Steph. And Steph is taller than Joseph. All right, that adds the name in, so now we just need to compare the height. Do we need to see these other heights? I don't think so. Let's just jump right in to the calculation. So we can take the person's height, which is on the height_inches column, and subtract from it lag, not the name this time, we want the height in inches by one over order by. We're going to sort it the same way. And let's call this as by this many inches. All right, and that got a little long, so let me wrap this on to multiple lines. So there is our alias right here. So we're going to take whatever the person's height is. We'll subtract whatever the value is of the height of the person below them. And we'll call it by this many inches. All right, let's go ahead and give that a run, and there is the result, and that looks pretty good. So Marcellus is taller than Steph by 2.13 inches and Steph is taller than Joseph by .03. And if we take a look at these numbers, 74.74 minus 74.71 gives us the .03 value right there. So it looks like the calculation checks out. So that is my solution to the challenge. I'll save this as challenge_five_complete.txt, and you can compare my results against yours.